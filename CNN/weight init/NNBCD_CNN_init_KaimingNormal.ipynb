{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ce8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.13.1\n",
      "Torchvision Version: 0.14.1\n",
      "GPU is available? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly.decomposition import tensor_train\n",
    "from tensorly.decomposition import tensor_train\n",
    "from tensorly import tt_to_tensor\n",
    "from tensorly.decomposition import matrix_product_state\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Torchvision Version:\", torchvision.__version__)\n",
    "print(\"GPU is available?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32135b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert to tensor and scale to [0, 1]\n",
    "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
    "mnist_trainset = datasets.MNIST('/home/c/cl237/', train=True, download=True, transform=ts)\n",
    "mnist_testset = datasets.MNIST(root='/home/c/cl237/', train=False, download=True, transform=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee30946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "train_subset_size = int(0.1 * len(mnist_trainset))\n",
    "test_subset_size = int(0.0999 * len(mnist_testset))\n",
    "train_indices = list(range(len(mnist_trainset)))\n",
    "test_indices = list(range(len(mnist_testset)))\n",
    "torch.manual_seed(10)\n",
    "\n",
    "seed_value = 10\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "np.random.shuffle(train_indices)\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "train_subset_indices = train_indices[:train_subset_size]\n",
    "test_subset_indices = test_indices[:test_subset_size]\n",
    "\n",
    "mnist_trainset = Subset(mnist_trainset, train_subset_indices)\n",
    "mnist_testset = Subset(mnist_testset, test_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc58bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d0 = mnist_trainset[0][0].size()[0]\n",
    "x_d1 = mnist_trainset[0][0].size()[1]\n",
    "x_d2 = mnist_trainset[0][0].size()[2]\n",
    "# ([1, 28, 28])\n",
    "N = x_d3 = len(mnist_trainset)\n",
    "K = 10\n",
    "x_train = torch.empty((N,x_d0*x_d1*x_d2), device=device)\n",
    "# (60000, 28*28)\n",
    "\n",
    "y_train = torch.empty(N, dtype=torch.long)\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "     x_train[i,:] = torch.reshape(mnist_trainset[i][0], (1, x_d0*x_d1*x_d2))\n",
    "     y_train[i] = mnist_trainset[i][1]\n",
    "x_train = torch.t(x_train)\n",
    "y_one_hot = torch.zeros(N, K).scatter_(1, torch.reshape(y_train, (N, 1)), 1)\n",
    "y_one_hot = torch.t(y_one_hot).to(device=device)\n",
    "y_train = y_train.to(device=device)\n",
    "\n",
    "# Manipulate test set\n",
    "N_test = x_d3_test = len(mnist_testset)\n",
    "x_test = torch.empty((N_test,x_d0*x_d1*x_d2), device=device)\n",
    "y_test = torch.empty(N_test, dtype=torch.long)\n",
    "for i in range(N_test):\n",
    "     x_test[i,:] = torch.reshape(mnist_testset[i][0], (1, x_d0*x_d1*x_d2))\n",
    "     y_test[i] = mnist_testset[i][1]\n",
    "x_test = torch.t(x_test)\n",
    "y_test_one_hot = torch.zeros(N_test, K).scatter_(1, torch.reshape(y_test, (N_test, 1)), 1)\n",
    "y_test_one_hot = torch.t(y_test_one_hot).to(device=device)\n",
    "y_test = y_test.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588d15ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28, 1, 6000])\n",
      "torch.Size([28, 28, 1, 999])\n"
     ]
    }
   ],
   "source": [
    "#### Reshape X to X-bar\n",
    "x_trainTensor = torch.reshape(x_train, (x_d1, x_d2, x_d0,-1))\n",
    "x_testTensor  = torch.reshape(x_test, (x_d1, x_d2, x_d0,-1))\n",
    "print(x_trainTensor.shape)\n",
    "print(x_testTensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033c0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputX_CNN(x_Tensor, filter_size, stride):\n",
    "    H, W, C, n = x_Tensor.size()\n",
    "    Hprime = torch.floor(torch.tensor((H-filter_size)/stride))+1\n",
    "    Hprime = Hprime.to(torch.int)\n",
    "    Wprime = torch.floor(torch.tensor((W-filter_size)/stride))+1\n",
    "    Wprime = Wprime.to(torch.int)\n",
    "    Xtranform = torch.zeros((Hprime * Wprime, filter_size * filter_size * C, n), device=device)\n",
    "    for i in range(n):\n",
    "      Data =  x_Tensor[:,:,:,i]\n",
    "      for hh in range(Hprime):\n",
    "        for ww in range(Wprime):\n",
    "          #  print(range(ww * stride, ww * stride + filter_size))\n",
    "            DataTemp = Data[range(hh * stride, hh * stride + filter_size), :,:]\n",
    "            DataTemp = DataTemp[:, range(ww * stride, ww * stride + filter_size), :]\n",
    "            Xtranform[Hprime * hh + ww, :, i] = torch.reshape(DataTemp, (1, filter_size * filter_size * C))\n",
    "    return Xtranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122f82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### from X-bar to X-bar-bar\n",
    "filter_size=3\n",
    "stride = 2\n",
    "x_trainTS = inputX_CNN(x_trainTensor,filter_size,stride)\n",
    "x_testTS  = inputX_CNN(x_testTensor,filter_size,stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd77b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updateWb_CNN(U, V, W, W_tensor_rec, alpha, rho,tau): \n",
    "    W_tensor_rec = torch.as_tensor(W_tensor_rec,device=device).float()\n",
    "    W_tensor2matrix = W_tensor_rec.reshape(W.shape)  \n",
    "    d,N = V.size()\n",
    "    I = torch.eye(N, device=device)\n",
    "    U_prime = torch.t(U).reshape(n*Hprime*Wprime,-1)\n",
    "    Wstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(V),V))+(alpha+tau)*I), rho*torch.mm(torch.t(V),U_prime)+alpha*W+tau*W_tensor2matrix)\n",
    "    return Wstar\n",
    "\n",
    "def updateWb_CNNorg(U, V, W, alpha, rho):\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(N, device=device)\n",
    "    #_, col_U = U.size()\n",
    "    U_prime = torch.t(U).reshape(n*Hprime*Wprime,-1)\n",
    "    Wstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(V),V))+alpha*I), rho*torch.mm(torch.t(V),U_prime)+alpha*W)\n",
    "    \n",
    "    return Wstar\n",
    "\n",
    "def updateV(U1,U2,W,b,rho,gamma):\n",
    "    _, d = W.size()\n",
    "    I = torch.eye(d, device=device) #Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "    U1 = nn.ReLU()(U1)\n",
    "    _, col_U2 = U2.size()\n",
    "    Vstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(W),W))+gamma*I), rho*torch.mm(torch.t(W),U2-b.repeat(1,col_U2))+gamma*U1)\n",
    "    return Vstar\n",
    "\n",
    "def updateWb_org(U, V, W, b, alpha, rho):\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(d, device=device)\n",
    "    _, col_U = U.size()\n",
    "    Wstar = torch.mm(alpha*W+rho*torch.mm(U-b.repeat(1,col_U),torch.t(V)),torch.inverse(alpha*I+rho*(torch.mm(V,torch.t(V)))))\n",
    "    bstar = 0*(alpha*b+rho*torch.sum(U-torch.mm(Wstar,V), dim=1).reshape(b.size()))/(rho*N+alpha)\n",
    "    return Wstar, bstar\n",
    "\n",
    "def updateWb(U, V, W, b, W_tensor_rec, alpha, rho,tau):\n",
    "    W_tensor_rec = torch.as_tensor(W_tensor_rec,device=device).float()\n",
    "    W_tensor2matrix = W_tensor_rec.reshape(W.shape)\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(d, device=device)\n",
    "    _, col_U = U.size()\n",
    "    Wstar = torch.mm(alpha*W+tau*W_tensor2matrix+rho*torch.mm(U-b.repeat(1,col_U),torch.t(V)),torch.inverse((alpha+tau)*I+rho*(torch.mm(V,torch.t(V)))))\n",
    "    bstar = 0*(alpha*b+rho*torch.sum(U-torch.mm(Wstar,V), dim=1).reshape(b.size()))/(rho*N+alpha)\n",
    "    return Wstar, bstar\n",
    "\n",
    "def relu_prox(a, b, gamma, d, N):\n",
    "    val = torch.empty(d,N, device=device)\n",
    "    x = (a+gamma*b)/(1+gamma)\n",
    "    y = torch.min(b,torch.zeros(d,N, device=device))\n",
    "\n",
    "    val = torch.where(a+gamma*b < 0, y, torch.zeros(d,N, device=device))\n",
    "    val = torch.where(((a+gamma*b >= 0) & (b >=0)) | ((a*(gamma-np.sqrt(gamma*(gamma+1))) <= gamma*b) & (b < 0)), x, val)\n",
    "    val = torch.where((-a <= gamma*b) & (gamma*b <= a*(gamma-np.sqrt(gamma*(gamma+1)))), b, val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "    W1 = 0.01*torch.randn(filter_size * filter_size * C, S, device=device)  ## How people usually initialize the CNN kernel?\n",
    "    #W1_torch_tensor = W1.reshape((filter_size, filter_size, C, 2,2,2,2,2)) ## TBD\n",
    "    #W1_tl_tensor = tl.tensor(W1_torch_tensor.cpu().numpy())\n",
    "    #factors1 = tensor_train(W1_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, 1))\n",
    "    #W1_tl_tensor_rec = tt_to_tensor(factors1)\n",
    "    b1 = 0*torch.ones(d1, 1, device=device) # 0 is stable\n",
    "\n",
    "\n",
    "    W2 = init.kaiming_normal_(torch.empty(d2, d1, device=device),a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    W2_torch_tensor = W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2)) # 2^10 and Hprime * Wprime  *2^5*1^3\n",
    "    W2_tl_tensor = tl.tensor(W2_torch_tensor.cpu().numpy())\n",
    "    factors2 = tensor_train(W2_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial,1))\n",
    "    # we have 9 rank_initial (r1,...,r9) here plus 2 of 1s (r0 & r10).\n",
    "    # factors: set of tensor cores\n",
    "    # http://tensorly.org/stable/user_guide/tensor_decomposition.html\n",
    "    # http://tensorly.org/stable/modules/generated/tensorly.decomposition.tensor_train.html#tensorly.decomposition.tensor_train\n",
    "    W2_tl_tensor_rec = tt_to_tensor(factors2)\n",
    "    # tt_to_tensor(factors):Re-assembles ‘factors’, which represent a tensor in TT format into the corresponding full tensor\n",
    "    #      facros: list of 3d-arrays tt-cores           output_tensor: ndarray\n",
    "    b2 = 0*torch.ones(d2, 1, device=device)\n",
    "\n",
    "\n",
    "    W3 = init.kaiming_normal_(torch.empty(d3, d2, device=device),a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    W3_torch_tensor = W3.reshape((4,4,4,4,4,4,4,4,4,4)) # 8 number of 4s, 2 number of 8s\n",
    "    W3_tl_tensor = tl.tensor(W3_torch_tensor.cpu().numpy())\n",
    "    factors3 = tensor_train(W3_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial,1))\n",
    "    W3_tl_tensor_rec = tt_to_tensor(factors3)\n",
    "    b3 = 0*torch.ones(d3, 1, device=device)\n",
    "\n",
    "    W4 = init.kaiming_normal_(torch.empty(d4, d3, device=device),a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    b4 = 0*torch.ones(d4, 1, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af396bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank= 220 tau= 0.2 gamma= 0.5 rho= 0.5 alpha 1\n",
      "Train on 6000 samples, validate on 999 samples\n",
      "Repeatition 1 Epoch 1 / 2000 \n",
      " - time: 5.950917482376099 - sq_loss: 542.9500732421875 - tot_loss: 863.1941393403337 - acc: 0.29 - val_acc: 0.2822822822822823\n",
      "Repeatition 1 Epoch 2 / 2000 \n",
      " - time: 5.642564058303833 - sq_loss: 241.3111572265625 - tot_loss: 493.68705853717404 - acc: 0.421 - val_acc: 0.42042042042042044\n",
      "Repeatition 1 Epoch 3 / 2000 \n",
      " - time: 5.632015228271484 - sq_loss: 120.87895202636719 - tot_loss: 296.38874887079 - acc: 0.5043333333333333 - val_acc: 0.5055055055055055\n",
      "Repeatition 1 Epoch 4 / 2000 \n",
      " - time: 5.6204938888549805 - sq_loss: 63.263763427734375 - tot_loss: 182.79723730385302 - acc: 0.5723333333333334 - val_acc: 0.5785785785785785\n",
      "Repeatition 1 Epoch 5 / 2000 \n",
      " - time: 5.608172655105591 - sq_loss: 33.651947021484375 - tot_loss: 116.10604887530206 - acc: 0.6215 - val_acc: 0.6256256256256256\n",
      "Repeatition 1 Epoch 6 / 2000 \n",
      " - time: 5.63688588142395 - sq_loss: 18.02716636657715 - tot_loss: 76.15292078703642 - acc: 0.6683333333333333 - val_acc: 0.6706706706706707\n",
      "Repeatition 1 Epoch 7 / 2000 \n",
      " - time: 5.59437370300293 - sq_loss: 9.701717376708984 - tot_loss: 51.615263648331165 - acc: 0.7066666666666667 - val_acc: 0.6946946946946947\n",
      "Repeatition 1 Epoch 8 / 2000 \n",
      " - time: 5.61024808883667 - sq_loss: 5.245548725128174 - tot_loss: 36.11167085245252 - acc: 0.7348333333333333 - val_acc: 0.7267267267267268\n",
      "Repeatition 1 Epoch 9 / 2000 \n",
      " - time: 5.6207146644592285 - sq_loss: 2.852287769317627 - tot_loss: 26.02025236710906 - acc: 0.7561666666666667 - val_acc: 0.7487487487487487\n",
      "Repeatition 1 Epoch 10 / 2000 \n",
      " - time: 5.6001904010772705 - sq_loss: 1.562070608139038 - tot_loss: 19.257830937951802 - acc: 0.7728333333333334 - val_acc: 0.7657657657657657\n",
      "Repeatition 1 Epoch 11 / 2000 \n",
      " - time: 5.637359142303467 - sq_loss: 0.8631811141967773 - tot_loss: 14.602812333777548 - acc: 0.788 - val_acc: 0.7857857857857858\n",
      "Repeatition 1 Epoch 12 / 2000 \n",
      " - time: 5.594808101654053 - sq_loss: 0.4823068678379059 - tot_loss: 11.321005097404123 - acc: 0.8026666666666666 - val_acc: 0.7977977977977978\n",
      "Repeatition 1 Epoch 13 / 2000 \n",
      " - time: 5.601751327514648 - sq_loss: 0.2731727361679077 - tot_loss: 8.958543911576271 - acc: 0.815 - val_acc: 0.8138138138138138\n",
      "Repeatition 1 Epoch 14 / 2000 \n",
      " - time: 5.618704557418823 - sq_loss: 0.15727950632572174 - tot_loss: 7.22663226723671 - acc: 0.8251666666666667 - val_acc: 0.8198198198198198\n",
      "Repeatition 1 Epoch 15 / 2000 \n",
      " - time: 5.620166540145874 - sq_loss: 0.09234320372343063 - tot_loss: 5.936127579398454 - acc: 0.8315 - val_acc: 0.8308308308308309\n",
      "Repeatition 1 Epoch 16 / 2000 \n",
      " - time: 5.61335015296936 - sq_loss: 0.055480342358350754 - tot_loss: 4.960061993449926 - acc: 0.84 - val_acc: 0.8378378378378378\n",
      "Repeatition 1 Epoch 17 / 2000 \n",
      " - time: 5.63236927986145 - sq_loss: 0.034230735152959824 - tot_loss: 4.211389166116715 - acc: 0.846 - val_acc: 0.8428428428428428\n",
      "Repeatition 1 Epoch 18 / 2000 \n",
      " - time: 5.6028242111206055 - sq_loss: 0.02176259644329548 - tot_loss: 3.629256532341242 - acc: 0.8498333333333333 - val_acc: 0.8488488488488488\n",
      "Repeatition 1 Epoch 19 / 2000 \n",
      " - time: 5.601571083068848 - sq_loss: 0.01429705135524273 - tot_loss: 3.1705748372711238 - acc: 0.8548333333333333 - val_acc: 0.8578578578578578\n",
      "Repeatition 1 Epoch 20 / 2000 \n",
      " - time: 5.606553792953491 - sq_loss: 0.009723955765366554 - tot_loss: 2.8043450989760457 - acc: 0.8571666666666666 - val_acc: 0.8608608608608609\n",
      "Repeatition 1 Epoch 21 / 2000 \n",
      " - time: 5.62003493309021 - sq_loss: 0.006851147394627333 - tot_loss: 2.5080604730173945 - acc: 0.8608333333333333 - val_acc: 0.8648648648648649\n",
      "Repeatition 1 Epoch 22 / 2000 \n",
      " - time: 5.591758489608765 - sq_loss: 0.0049965111538767815 - tot_loss: 2.265254273544997 - acc: 0.865 - val_acc: 0.8658658658658659\n",
      "Repeatition 1 Epoch 23 / 2000 \n",
      " - time: 5.61055064201355 - sq_loss: 0.0037639557849615812 - tot_loss: 2.0637398946564645 - acc: 0.868 - val_acc: 0.8718718718718719\n",
      "Repeatition 1 Epoch 24 / 2000 \n",
      " - time: 5.61870002746582 - sq_loss: 0.0029200215358287096 - tot_loss: 1.8944490077905358 - acc: 0.8701666666666666 - val_acc: 0.8758758758758759\n",
      "Repeatition 1 Epoch 25 / 2000 \n",
      " - time: 5.774094343185425 - sq_loss: 0.0023249376099556684 - tot_loss: 1.7505817546974867 - acc: 0.872 - val_acc: 0.8758758758758759\n",
      "Repeatition 1 Epoch 26 / 2000 \n",
      " - time: 5.872032165527344 - sq_loss: 0.0018930754158645868 - tot_loss: 1.6269853513222188 - acc: 0.8731666666666666 - val_acc: 0.8778778778778779\n",
      "Repeatition 1 Epoch 27 / 2000 \n",
      " - time: 5.995197534561157 - sq_loss: 0.0015710181323811412 - tot_loss: 1.519719733321108 - acc: 0.876 - val_acc: 0.8778778778778779\n",
      "Repeatition 1 Epoch 28 / 2000 \n",
      " - time: 5.698085784912109 - sq_loss: 0.0013248906470835209 - tot_loss: 1.4257570444140584 - acc: 0.8775 - val_acc: 0.8828828828828829\n",
      "Repeatition 1 Epoch 29 / 2000 \n",
      " - time: 5.61717963218689 - sq_loss: 0.0011326582171022892 - tot_loss: 1.3427507313899696 - acc: 0.8791666666666667 - val_acc: 0.8838838838838838\n",
      "Repeatition 1 Epoch 30 / 2000 \n",
      " - time: 5.576929092407227 - sq_loss: 0.0009795608930289745 - tot_loss: 1.268857349222526 - acc: 0.8796666666666667 - val_acc: 0.8818818818818819\n",
      "Repeatition 1 Epoch 31 / 2000 \n",
      " - time: 5.596866130828857 - sq_loss: 0.0008556419052183628 - tot_loss: 1.2026138294953854 - acc: 0.883 - val_acc: 0.8828828828828829\n",
      "Repeatition 1 Epoch 32 / 2000 \n",
      " - time: 5.587104320526123 - sq_loss: 0.0007538968930020928 - tot_loss: 1.1428582461318 - acc: 0.8845 - val_acc: 0.8848848848848849\n",
      "Repeatition 1 Epoch 33 / 2000 \n",
      " - time: 5.632053852081299 - sq_loss: 0.0006693293107673526 - tot_loss: 1.0886571778682992 - acc: 0.8878333333333334 - val_acc: 0.8858858858858859\n",
      "Repeatition 1 Epoch 34 / 2000 \n",
      " - time: 5.5842132568359375 - sq_loss: 0.0005983641021884978 - tot_loss: 1.039252232585568 - acc: 0.89 - val_acc: 0.8908908908908909\n",
      "Repeatition 1 Epoch 35 / 2000 \n",
      " - time: 5.583750486373901 - sq_loss: 0.0005381926894187927 - tot_loss: 0.9940147938439622 - acc: 0.8911666666666667 - val_acc: 0.8918918918918919\n",
      "Repeatition 1 Epoch 36 / 2000 \n",
      " - time: 5.608516693115234 - sq_loss: 0.00048678420716896653 - tot_loss: 0.9524250511196443 - acc: 0.8928333333333334 - val_acc: 0.8938938938938938\n",
      "Repeatition 1 Epoch 37 / 2000 \n",
      " - time: 5.59990382194519 - sq_loss: 0.0004426136438269168 - tot_loss: 0.9140471733000596 - acc: 0.894 - val_acc: 0.8958958958958959\n",
      "Repeatition 1 Epoch 38 / 2000 \n",
      " - time: 5.568000793457031 - sq_loss: 0.0004044322413392365 - tot_loss: 0.8785175878205337 - acc: 0.8946666666666667 - val_acc: 0.8978978978978979\n",
      "Repeatition 1 Epoch 39 / 2000 \n",
      " - time: 5.591102600097656 - sq_loss: 0.00037124528898857534 - tot_loss: 0.8455268843274099 - acc: 0.8963333333333333 - val_acc: 0.8988988988988988\n",
      "Repeatition 1 Epoch 40 / 2000 \n",
      " - time: 5.581860303878784 - sq_loss: 0.00034225310082547367 - tot_loss: 0.8148116650001611 - acc: 0.8976666666666666 - val_acc: 0.8988988988988988\n",
      "Repeatition 1 Epoch 41 / 2000 \n",
      " - time: 5.59281849861145 - sq_loss: 0.0003167865506839007 - tot_loss: 0.7861441420565825 - acc: 0.8988333333333334 - val_acc: 0.9009009009009009\n",
      "Repeatition 1 Epoch 42 / 2000 \n",
      " - time: 5.672967910766602 - sq_loss: 0.0002943399304058403 - tot_loss: 0.7593255268700887 - acc: 0.8998333333333334 - val_acc: 0.9019019019019019\n",
      "Repeatition 1 Epoch 43 / 2000 \n",
      " - time: 5.602341175079346 - sq_loss: 0.00027446786407381296 - tot_loss: 0.7341811172082089 - acc: 0.9015 - val_acc: 0.9029029029029029\n",
      "Repeatition 1 Epoch 44 / 2000 \n",
      " - time: 5.618796348571777 - sq_loss: 0.0002568163035903126 - tot_loss: 0.7105623658339028 - acc: 0.902 - val_acc: 0.9029029029029029\n",
      "Repeatition 1 Epoch 45 / 2000 \n",
      " - time: 5.602450847625732 - sq_loss: 0.000241072426433675 - tot_loss: 0.6883365631656488 - acc: 0.9026666666666666 - val_acc: 0.9019019019019019\n",
      "Repeatition 1 Epoch 46 / 2000 \n",
      " - time: 5.606166362762451 - sq_loss: 0.0002269869000883773 - tot_loss: 0.6673865679913433 - acc: 0.9035 - val_acc: 0.9029029029029029\n",
      "Repeatition 1 Epoch 47 / 2000 \n",
      " - time: 5.5907816886901855 - sq_loss: 0.00021434022346511483 - tot_loss: 0.6476074528647587 - acc: 0.904 - val_acc: 0.9019019019019019\n",
      "Repeatition 1 Epoch 48 / 2000 \n",
      " - time: 5.5995872020721436 - sq_loss: 0.00020293553825467825 - tot_loss: 0.628907734004315 - acc: 0.9046666666666666 - val_acc: 0.9029029029029029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeatition 1 Epoch 49 / 2000 \n",
      " - time: 5.580448627471924 - sq_loss: 0.0001926186087075621 - tot_loss: 0.6112024812202436 - acc: 0.905 - val_acc: 0.9039039039039038\n",
      "Repeatition 1 Epoch 50 / 2000 \n",
      " - time: 5.595702171325684 - sq_loss: 0.00018325838027521968 - tot_loss: 0.5944153199438006 - acc: 0.9065 - val_acc: 0.9039039039039038\n",
      "Repeatition 1 Epoch 51 / 2000 \n",
      " - time: 5.606915235519409 - sq_loss: 0.000174726388650015 - tot_loss: 0.5784801504167263 - acc: 0.9066666666666666 - val_acc: 0.9039039039039038\n",
      "Repeatition 1 Epoch 52 / 2000 \n",
      " - time: 5.935134410858154 - sq_loss: 0.0001669601770117879 - tot_loss: 0.5633365927496925 - acc: 0.9071666666666667 - val_acc: 0.9049049049049049\n",
      "Repeatition 1 Epoch 53 / 2000 \n",
      " - time: 5.648139715194702 - sq_loss: 0.00015984858328010887 - tot_loss: 0.5489295928826323 - acc: 0.9076666666666666 - val_acc: 0.9069069069069069\n",
      "Repeatition 1 Epoch 54 / 2000 \n",
      " - time: 5.59767484664917 - sq_loss: 0.0001533022295916453 - tot_loss: 0.535210158539121 - acc: 0.9078333333333334 - val_acc: 0.9079079079079079\n",
      "Repeatition 1 Epoch 55 / 2000 \n",
      " - time: 5.591058969497681 - sq_loss: 0.00014727222151122987 - tot_loss: 0.5221322246536146 - acc: 0.9085 - val_acc: 0.9099099099099099\n",
      "Repeatition 1 Epoch 56 / 2000 \n",
      " - time: 5.6135640144348145 - sq_loss: 0.00014173210365697742 - tot_loss: 0.5096534109907225 - acc: 0.9093333333333333 - val_acc: 0.9109109109109109\n",
      "Repeatition 1 Epoch 57 / 2000 \n",
      " - time: 5.616800546646118 - sq_loss: 0.00013659540854860097 - tot_loss: 0.497735868676682 - acc: 0.9096666666666666 - val_acc: 0.9109109109109109\n",
      "Repeatition 1 Epoch 58 / 2000 \n",
      " - time: 5.643405199050903 - sq_loss: 0.00013182018301449716 - tot_loss: 0.4863440785789862 - acc: 0.9101666666666667 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 59 / 2000 \n",
      " - time: 5.607564210891724 - sq_loss: 0.00012738248915411532 - tot_loss: 0.47544460131321103 - acc: 0.9108333333333334 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 60 / 2000 \n",
      " - time: 5.571312665939331 - sq_loss: 0.0001232428039656952 - tot_loss: 0.46500699213647745 - acc: 0.9115 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 61 / 2000 \n",
      " - time: 5.582070589065552 - sq_loss: 0.00011938712123082951 - tot_loss: 0.4550057974309311 - acc: 0.9116666666666666 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 62 / 2000 \n",
      " - time: 5.575458288192749 - sq_loss: 0.00011576755787245929 - tot_loss: 0.4454151540645398 - acc: 0.9125 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 63 / 2000 \n",
      " - time: 5.5879645347595215 - sq_loss: 0.00011238876322750002 - tot_loss: 0.4362113758834312 - acc: 0.9131666666666667 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 64 / 2000 \n",
      " - time: 5.602507591247559 - sq_loss: 0.00010919551277766004 - tot_loss: 0.4273730066735879 - acc: 0.914 - val_acc: 0.9119119119119119\n",
      "Repeatition 1 Epoch 65 / 2000 \n",
      " - time: 5.6117846965789795 - sq_loss: 0.00010619136446621269 - tot_loss: 0.41888123769022056 - acc: 0.9143333333333333 - val_acc: 0.9119119119119119\n",
      "Repeatition 1 Epoch 66 / 2000 \n",
      " - time: 5.631491661071777 - sq_loss: 0.00010337068670196459 - tot_loss: 0.4107165525652817 - acc: 0.9146666666666666 - val_acc: 0.9119119119119119\n",
      "Repeatition 1 Epoch 67 / 2000 \n",
      " - time: 5.606673002243042 - sq_loss: 0.00010070006101159379 - tot_loss: 0.4028601449492271 - acc: 0.915 - val_acc: 0.9119119119119119\n",
      "Repeatition 1 Epoch 68 / 2000 \n",
      " - time: 5.581589937210083 - sq_loss: 9.819020488066599e-05 - tot_loss: 0.39529621117835634 - acc: 0.9155 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 69 / 2000 \n",
      " - time: 5.600322723388672 - sq_loss: 9.579848847351968e-05 - tot_loss: 0.38800975591875614 - acc: 0.9158333333333334 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 70 / 2000 \n",
      " - time: 5.613736867904663 - sq_loss: 9.35280040721409e-05 - tot_loss: 0.3809848996446817 - acc: 0.9161666666666667 - val_acc: 0.914914914914915\n",
      "Repeatition 1 Epoch 71 / 2000 \n",
      " - time: 5.59528112411499 - sq_loss: 9.13681578822434e-05 - tot_loss: 0.37421039781183935 - acc: 0.9161666666666667 - val_acc: 0.914914914914915\n",
      "Repeatition 1 Epoch 72 / 2000 \n",
      " - time: 5.599208831787109 - sq_loss: 8.930413605412468e-05 - tot_loss: 0.3676749265039689 - acc: 0.917 - val_acc: 0.9159159159159159\n",
      "Repeatition 1 Epoch 73 / 2000 \n",
      " - time: 5.56604528427124 - sq_loss: 8.733669528737664e-05 - tot_loss: 0.36136598758748734 - acc: 0.9171666666666667 - val_acc: 0.9159159159159159\n",
      "Repeatition 1 Epoch 74 / 2000 \n",
      " - time: 5.588122367858887 - sq_loss: 8.54577447171323e-05 - tot_loss: 0.3552731812160346 - acc: 0.9176666666666666 - val_acc: 0.9159159159159159\n",
      "Repeatition 1 Epoch 75 / 2000 \n",
      " - time: 5.6098456382751465 - sq_loss: 8.365810936084017e-05 - tot_loss: 0.3493846294397372 - acc: 0.918 - val_acc: 0.9159159159159159\n",
      "Repeatition 1 Epoch 76 / 2000 \n",
      " - time: 5.59021782875061 - sq_loss: 8.193946268875152e-05 - tot_loss: 0.3436912462319015 - acc: 0.9183333333333333 - val_acc: 0.9159159159159159\n",
      "Repeatition 1 Epoch 77 / 2000 \n",
      " - time: 5.57909083366394 - sq_loss: 8.029101445572451e-05 - tot_loss: 0.33818379217555045 - acc: 0.9185 - val_acc: 0.9159159159159159\n",
      "Repeatition 1 Epoch 78 / 2000 \n",
      " - time: 5.663642406463623 - sq_loss: 7.871542038628832e-05 - tot_loss: 0.3328538543559261 - acc: 0.919 - val_acc: 0.9159159159159159\n",
      "Repeatition 1 Epoch 79 / 2000 \n",
      " - time: 6.034210920333862 - sq_loss: 7.719414861639962e-05 - tot_loss: 0.3276938621289446 - acc: 0.9195 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 80 / 2000 \n",
      " - time: 6.0135016441345215 - sq_loss: 7.573181937914342e-05 - tot_loss: 0.32269652511167807 - acc: 0.9198333333333333 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 81 / 2000 \n",
      " - time: 5.785267114639282 - sq_loss: 7.434131111949682e-05 - tot_loss: 0.3178543250716757 - acc: 0.9203333333333333 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 82 / 2000 \n",
      " - time: 5.779073476791382 - sq_loss: 7.299958087969571e-05 - tot_loss: 0.3131603597139474 - acc: 0.9203333333333333 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 83 / 2000 \n",
      " - time: 5.800329208374023 - sq_loss: 7.169109449023381e-05 - tot_loss: 0.3086088801836013 - acc: 0.921 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 84 / 2000 \n",
      " - time: 5.743419885635376 - sq_loss: 7.0435598900076e-05 - tot_loss: 0.3041932653504773 - acc: 0.9211666666666667 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 85 / 2000 \n",
      " - time: 5.7509605884552 - sq_loss: 6.921305612195283e-05 - tot_loss: 0.29990714576269967 - acc: 0.9218333333333333 - val_acc: 0.9179179179179179\n",
      "Repeatition 1 Epoch 86 / 2000 \n",
      " - time: 5.753148317337036 - sq_loss: 6.803605356253684e-05 - tot_loss: 0.29574572408164385 - acc: 0.9221666666666667 - val_acc: 0.9179179179179179\n",
      "Repeatition 1 Epoch 87 / 2000 \n",
      " - time: 5.794983386993408 - sq_loss: 6.68985812808387e-05 - tot_loss: 0.2917041369117215 - acc: 0.9226666666666666 - val_acc: 0.9179179179179179\n",
      "Repeatition 1 Epoch 88 / 2000 \n",
      " - time: 5.816438436508179 - sq_loss: 6.579676846740767e-05 - tot_loss: 0.28777735983749153 - acc: 0.923 - val_acc: 0.9179179179179179\n",
      "Repeatition 1 Epoch 89 / 2000 \n",
      " - time: 5.786679744720459 - sq_loss: 6.473548273788765e-05 - tot_loss: 0.2839601654137368 - acc: 0.9233333333333333 - val_acc: 0.918918918918919\n",
      "Repeatition 1 Epoch 90 / 2000 \n",
      " - time: 5.787225246429443 - sq_loss: 6.370704068103805e-05 - tot_loss: 0.28024819206184476 - acc: 0.9243333333333333 - val_acc: 0.9209209209209209\n",
      "Repeatition 1 Epoch 91 / 2000 \n",
      " - time: 5.783008098602295 - sq_loss: 6.272504106163979e-05 - tot_loss: 0.2766371987992897 - acc: 0.925 - val_acc: 0.9209209209209209\n",
      "Repeatition 1 Epoch 92 / 2000 \n",
      " - time: 5.728634834289551 - sq_loss: 6.176759779918939e-05 - tot_loss: 0.2731234497594414 - acc: 0.9253333333333333 - val_acc: 0.9209209209209209\n"
     ]
    }
   ],
   "source": [
    "niter = 2000\n",
    "rank = 220\n",
    "tau = 0.2\n",
    "gamma = 0.5\n",
    "alpha = 1\n",
    "rho = 0.5\n",
    "\n",
    "print (\"rank=\",rank, \"tau=\",tau, \"gamma=\",gamma, \"rho=\",rho, \"alpha\",alpha)\n",
    "\n",
    "loss1 = np.empty(niter)\n",
    "loss2 = np.empty(niter)\n",
    "accuracy_train = np.empty(niter)\n",
    "accuracy_test = np.empty(niter)\n",
    "time1 = np.empty(niter)\n",
    "\n",
    "results = torch.zeros(1, 5, niter)\n",
    "\n",
    "S = 32 ### number of filters 2^5\n",
    "H, W, C, n = x_trainTensor.size()   # n is the same thing as N\n",
    "Hprime = torch.floor(torch.tensor((H-filter_size)/stride))+1\n",
    "Hprime = Hprime.to(torch.int)\n",
    "Wprime = torch.floor(torch.tensor((W-filter_size)/stride))+1\n",
    "Wprime = Wprime.to(torch.int)\n",
    "\n",
    "\n",
    "\n",
    "for Out_iter in range(1):\n",
    "    rank_initial = 700\n",
    "    seed = 10 + 10*Out_iter\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    d1 =  Hprime * Wprime * S\n",
    "    d2 =  1024\n",
    "    d3 =  1024\n",
    "    d4 =  10\n",
    "\n",
    "\n",
    "    W1 = 0.01*torch.randn(filter_size * filter_size * C, S, device=device)\n",
    "    b1 = 0*torch.ones(d1, 1, device=device) # 0 is stable\n",
    "\n",
    "\n",
    "    W2 = init.kaiming_normal_(torch.empty(d2, d1, device=device),a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    W2_torch_tensor = W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2)) \n",
    "    W2_tl_tensor = tl.tensor(W2_torch_tensor.cpu().numpy())\n",
    "    factors2 = tensor_train(W2_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial,1))\n",
    "    W2_tl_tensor_rec = tt_to_tensor(factors2)\n",
    "    b2 = 0*torch.ones(d2, 1, device=device)\n",
    "\n",
    "\n",
    "    W3 = init.kaiming_normal_(torch.empty(d3, d2, device=device),a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    W3_torch_tensor = W3.reshape((4,4,4,4,4,4,4,4,4,4)) \n",
    "    W3_tl_tensor = tl.tensor(W3_torch_tensor.cpu().numpy())\n",
    "    factors3 = tensor_train(W3_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial,1))\n",
    "    W3_tl_tensor_rec = tt_to_tensor(factors3)\n",
    "    b3 = 0*torch.ones(d3, 1, device=device)\n",
    "\n",
    "    W4 = init.kaiming_normal_(torch.empty(d4, d3, device=device),a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    b4 = 0*torch.ones(d4, 1, device=device)\n",
    "\n",
    "    x_trainTS1 = torch.reshape(x_trainTS, (-1,n))\n",
    "    x_trainTS1 = torch.t(x_trainTS1)\n",
    "    x_trainTS1 = torch.reshape(x_trainTS1, (-1, filter_size * filter_size * C))   ### this is X-bar-bar'\n",
    "    U1prime = torch.matmul(x_trainTS1, W1)\n",
    "    U1prime = torch.reshape(U1prime, (n,-1))\n",
    "    U1 = torch.t(U1prime)\n",
    "\n",
    "    V1 = nn.ReLU()(U1)\n",
    "    U2 = torch.addmm(b2.repeat(1, N), W2, V1)\n",
    "    V2 = nn.ReLU()(U2)\n",
    "    U3 = torch.addmm(b3.repeat(1, N), W3, V2)\n",
    "    V3 = nn.ReLU()(U3)\n",
    "    U4 = torch.addmm(b4.repeat(1, N), W4, V3)\n",
    "    V4 = U4\n",
    "\n",
    "    x_testTS1 = torch.reshape(x_testTS, (-1, N_test))\n",
    "    x_testTS1 = torch.t(x_testTS1)\n",
    "    x_testTS1 = torch.reshape(x_testTS1, (-1, filter_size * filter_size * C))   ### this is X-bar-bar' test\n",
    "\n",
    "\n",
    "    # Iterations\n",
    "    print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
    "    for k in range(niter):\n",
    "        start = time.time()\n",
    "\n",
    "  # update for last layer\n",
    "        # update V4\n",
    "        V4 = (y_one_hot + gamma*U4 + alpha*V4)/(1 + gamma + alpha)\n",
    "\n",
    "        # update U4\n",
    "        U4 = (gamma*V4 + rho*(torch.mm(W4,V3) + b4.repeat(1,N)))/(gamma + rho)\n",
    "\n",
    "        # update W4 and b4\n",
    "        W4, b4 = updateWb_org(U4,V3,W4,b4,alpha,rho)\n",
    "\n",
    "\n",
    "  # update for 3nd layer\n",
    "        # update V3\n",
    "        V3 = updateV(U3,U4,W4,b4,rho,gamma)\n",
    "\n",
    "        # update U3\n",
    "        U3 = relu_prox(V3,(rho*torch.addmm(b3.repeat(1,N), W3, V2) + alpha*U3)/(rho + alpha),(rho + alpha)/gamma,d3,N)\n",
    "\n",
    "        # update W3 and b3\n",
    "        W3, b3 = updateWb(U3,V2,W3,b3,W3_tl_tensor_rec, alpha,rho,tau)\n",
    "\n",
    "        # G update (TTD)\n",
    "        W3_torch_tensor = W3.reshape((4,4,4,4,4,4,4,4,4,4))\n",
    "        W3_tl_tensor = tl.tensor(W3_torch_tensor.cpu().numpy())  # transfer tensorly package\n",
    "        factors3 = tensor_train(W3_tl_tensor, (1,rank,rank,rank,rank,rank,rank,rank,rank,rank,1))\n",
    "        #set of tensor cores\n",
    "        W3_tl_tensor_rec = tt_to_tensor(factors3)\n",
    "\n",
    "\n",
    "  # update for 2nd layer\n",
    "        # update V2\n",
    "        V2 = updateV(U2,U3,W3,b3,rho,gamma)\n",
    "\n",
    "        # update U2\n",
    "        U2 = relu_prox(V2,(rho*torch.addmm(b2.repeat(1,N), W2, V1) + alpha*U2)/(rho + alpha),(rho + alpha)/gamma,d2,N)\n",
    "\n",
    "        # update W2 and b2\n",
    "        W2, b2 = updateWb(U2,V1,W2,b2,W2_tl_tensor_rec, alpha,rho,tau)\n",
    "\n",
    "        # G update (TTD)\n",
    "        W2_torch_tensor = W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2))\n",
    "        W2_tl_tensor = tl.tensor(W2_torch_tensor.cpu().numpy())\n",
    "        factors2 = tensor_train(W2_tl_tensor, (1,rank,rank,rank,rank,rank,rank,rank,rank,rank,1))\n",
    "        W2_tl_tensor_rec = tt_to_tensor(factors2)\n",
    "\n",
    "\n",
    " # update for 1st layer\n",
    "        # update V1\n",
    "        V1 = updateV(U1,U2,W2,b2,rho,gamma)\n",
    "\n",
    "        XprimeW = torch.reshape(torch.matmul(x_trainTS1, W1), (n,-1))\n",
    "        XprimeWtranspose = torch.t(XprimeW)\n",
    "        # update U1\n",
    "        U1 = relu_prox(V1,(rho*XprimeWtranspose + alpha*U1)/(rho + alpha),(rho + alpha)/gamma,d1,N)\n",
    "\n",
    "        # update W1 and b1\n",
    "        W1 = updateWb_CNNorg(U1,x_trainTS1,W1,alpha,rho)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # prediction for trainning data\n",
    "        XprimeW = torch.reshape(torch.matmul(x_trainTS1, torch.as_tensor(W1,device=device).reshape((filter_size * filter_size * C, S)).float()), (n,-1))\n",
    "        XprimeWtranspose = torch.t(XprimeW)\n",
    "        a1_train = nn.ReLU()(XprimeWtranspose)\n",
    "        a2_train = nn.ReLU()(torch.addmm(b2.repeat(1, N), torch.as_tensor(W2_tl_tensor_rec,device=device).reshape((d2, d1)).float(), a1_train))\n",
    "        a3_train = nn.ReLU()(torch.addmm(b3.repeat(1, N), torch.as_tensor(W3_tl_tensor_rec,device=device).reshape((d3, d2)), a2_train))\n",
    "        pred = torch.argmax(torch.addmm(b4.repeat(1, N), W4, a3_train), dim=0)\n",
    "\n",
    "  #Prediction for test data\n",
    "        XprimeWtest = torch.reshape(torch.matmul(x_testTS1, torch.as_tensor(W1,device=device).reshape((filter_size * filter_size * C, S)).float()), (N_test,-1))\n",
    "        XprimeWtesttranspose = torch.t(XprimeWtest)\n",
    "        a1_test = nn.ReLU()(XprimeWtesttranspose) \n",
    "        a2_test = nn.ReLU()(torch.addmm(b2.repeat(1, N_test), torch.as_tensor(W2_tl_tensor_rec,device=device).reshape((d2, d1)).float(), a1_test))\n",
    "        a3_test = nn.ReLU()(torch.addmm(b3.repeat(1, N_test), torch.as_tensor(W3_tl_tensor_rec,device=device).reshape((d3, d2)), a2_test))\n",
    "        pred_test = torch.argmax(torch.addmm(b4.repeat(1, N_test), W4, a3_test), dim=0)\n",
    "\n",
    "\n",
    "    #emperical loss\n",
    "        loss1[k] = gamma/2*torch.pow(torch.dist(V4,y_one_hot,2),2).cpu().numpy()\n",
    "\n",
    "        # Eq (5) in paper\n",
    "        loss2[k] = loss1[k] + rho/2*torch.pow(torch.dist(XprimeWtranspose,U1,2),2).cpu().numpy() \\\n",
    "        +rho/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
    "        +rho/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy() \\\n",
    "        +rho/2*torch.pow(torch.dist(torch.addmm(b4.repeat(1,N), W4, V3),U4,2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V1,nn.ReLU()(U1),2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V2,nn.ReLU()(U2),2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V3,nn.ReLU()(U3),2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V4,U4,2),2).cpu().numpy() \\\n",
    "        +tau/2*torch.pow(torch.dist(W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2)),torch.as_tensor(W2_tl_tensor_rec,device=device).float(),2),2).cpu().numpy() \\\n",
    "        +tau/2*torch.pow(torch.dist(W3.reshape((4,4,4,4,4,4,4,4,4,4)),torch.as_tensor(W3_tl_tensor_rec,device=device).float(),2),2).cpu().numpy() \\\n",
    "\n",
    "        # compute training accuracy\n",
    "        correct_train = pred == y_train\n",
    "        accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
    "\n",
    "        # compute validation accuracy\n",
    "        correct_test = pred_test == y_test\n",
    "        accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
    "\n",
    "        # compute training time\n",
    "        stop = time.time()\n",
    "        duration = stop - start\n",
    "        time1[k] = duration\n",
    "\n",
    "        # print results\n",
    "        print('Repeatition', Out_iter + 1, 'Epoch', k + 1, '/', niter, '\\n',\n",
    "              '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:', loss2[k],\n",
    "              '-', 'acc:', accuracy_train[k], '-', 'val_acc:', accuracy_test[k])\n",
    "\n",
    "##############\n",
    "############## compute CR\n",
    "    factors2_shape=[f.shape for f in factors2]\n",
    "    Sum_of_variables_factors2=sum(list(x*y*z for x,y,z in factors2_shape))\n",
    "    factors3_shape=[f.shape for f in factors3]\n",
    "    Sum_of_variables_factors3=sum(list(x*y*z for x,y,z in factors3_shape))\n",
    "    total_variabels=Sum_of_variables_factors2+Sum_of_variables_factors3\n",
    "\n",
    "    layer2_CR = Sum_of_variables_factors2/(d1*d2).item()\n",
    "    layer3_CR = Sum_of_variables_factors3/(d2*d3)\n",
    "    Compressedlayers_CR = total_variabels/(d1*d2+d2*d3).item()\n",
    "    Compressedlayers_CR2 = (total_variabels+d3*d4)/(d1*d2+d2*d3+d3*d4).item()\n",
    "\n",
    "\n",
    "\n",
    "    results[Out_iter,0,:] = torch.tensor(loss1)\n",
    "    results[Out_iter,1,:] = torch.tensor(loss2)\n",
    "    results[Out_iter,2,:] = torch.tensor(accuracy_train)\n",
    "    results[Out_iter,3,:] = torch.tensor(accuracy_test)\n",
    "    results[Out_iter,4,:] = torch.tensor(time1)\n",
    "    CR=(layer2_CR,layer3_CR,Compressedlayers_CR,Compressedlayers_CR2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338341d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
