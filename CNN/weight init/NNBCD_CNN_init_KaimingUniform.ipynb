{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ce8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.13.1\n",
      "Torchvision Version: 0.14.1\n",
      "GPU is available? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly.decomposition import tensor_train\n",
    "from tensorly.decomposition import tensor_train\n",
    "from tensorly import tt_to_tensor\n",
    "from tensorly.decomposition import matrix_product_state\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Torchvision Version:\", torchvision.__version__)\n",
    "print(\"GPU is available?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32135b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert to tensor and scale to [0, 1]\n",
    "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
    "mnist_trainset = datasets.MNIST('/home/c/cl237/', train=True, download=True, transform=ts)\n",
    "mnist_testset = datasets.MNIST(root='/home/c/cl237/', train=False, download=True, transform=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee30946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "train_subset_size = int(0.1 * len(mnist_trainset))\n",
    "test_subset_size = int(0.0999 * len(mnist_testset))\n",
    "train_indices = list(range(len(mnist_trainset)))\n",
    "test_indices = list(range(len(mnist_testset)))\n",
    "torch.manual_seed(10)\n",
    "\n",
    "seed_value = 10\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "np.random.shuffle(train_indices)\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "train_subset_indices = train_indices[:train_subset_size]\n",
    "test_subset_indices = test_indices[:test_subset_size]\n",
    "\n",
    "mnist_trainset = Subset(mnist_trainset, train_subset_indices)\n",
    "mnist_testset = Subset(mnist_testset, test_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc58bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d0 = mnist_trainset[0][0].size()[0]\n",
    "x_d1 = mnist_trainset[0][0].size()[1]\n",
    "x_d2 = mnist_trainset[0][0].size()[2]\n",
    "# ([1, 28, 28])\n",
    "N = x_d3 = len(mnist_trainset)\n",
    "K = 10\n",
    "x_train = torch.empty((N,x_d0*x_d1*x_d2), device=device)\n",
    "# (60000, 28*28)\n",
    "\n",
    "y_train = torch.empty(N, dtype=torch.long)\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "     x_train[i,:] = torch.reshape(mnist_trainset[i][0], (1, x_d0*x_d1*x_d2))\n",
    "     y_train[i] = mnist_trainset[i][1]\n",
    "x_train = torch.t(x_train)\n",
    "y_one_hot = torch.zeros(N, K).scatter_(1, torch.reshape(y_train, (N, 1)), 1)\n",
    "y_one_hot = torch.t(y_one_hot).to(device=device)\n",
    "y_train = y_train.to(device=device)\n",
    "\n",
    "# Manipulate test set\n",
    "N_test = x_d3_test = len(mnist_testset)\n",
    "x_test = torch.empty((N_test,x_d0*x_d1*x_d2), device=device)\n",
    "y_test = torch.empty(N_test, dtype=torch.long)\n",
    "for i in range(N_test):\n",
    "     x_test[i,:] = torch.reshape(mnist_testset[i][0], (1, x_d0*x_d1*x_d2))\n",
    "     y_test[i] = mnist_testset[i][1]\n",
    "x_test = torch.t(x_test)\n",
    "y_test_one_hot = torch.zeros(N_test, K).scatter_(1, torch.reshape(y_test, (N_test, 1)), 1)\n",
    "y_test_one_hot = torch.t(y_test_one_hot).to(device=device)\n",
    "y_test = y_test.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588d15ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28, 1, 6000])\n",
      "torch.Size([28, 28, 1, 999])\n"
     ]
    }
   ],
   "source": [
    "#### Reshape X to X-bar\n",
    "x_trainTensor = torch.reshape(x_train, (x_d1, x_d2, x_d0,-1))\n",
    "x_testTensor  = torch.reshape(x_test, (x_d1, x_d2, x_d0,-1))\n",
    "print(x_trainTensor.shape)\n",
    "print(x_testTensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033c0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputX_CNN(x_Tensor, filter_size, stride):\n",
    "    H, W, C, n = x_Tensor.size()\n",
    "    Hprime = torch.floor(torch.tensor((H-filter_size)/stride))+1\n",
    "    Hprime = Hprime.to(torch.int)\n",
    "    Wprime = torch.floor(torch.tensor((W-filter_size)/stride))+1\n",
    "    Wprime = Wprime.to(torch.int)\n",
    "    Xtranform = torch.zeros((Hprime * Wprime, filter_size * filter_size * C, n), device=device)\n",
    "    for i in range(n):\n",
    "      Data =  x_Tensor[:,:,:,i]\n",
    "      for hh in range(Hprime):\n",
    "        for ww in range(Wprime):\n",
    "          #  print(range(ww * stride, ww * stride + filter_size))\n",
    "            DataTemp = Data[range(hh * stride, hh * stride + filter_size), :,:]\n",
    "            DataTemp = DataTemp[:, range(ww * stride, ww * stride + filter_size), :]\n",
    "            Xtranform[Hprime * hh + ww, :, i] = torch.reshape(DataTemp, (1, filter_size * filter_size * C))\n",
    "    return Xtranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122f82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### from X-bar to X-bar-bar\n",
    "filter_size=3\n",
    "stride = 2\n",
    "x_trainTS = inputX_CNN(x_trainTensor,filter_size,stride)\n",
    "x_testTS  = inputX_CNN(x_testTensor,filter_size,stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd77b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updateWb_CNN(U, V, W, W_tensor_rec, alpha, rho,tau): \n",
    "    W_tensor_rec = torch.as_tensor(W_tensor_rec,device=device).float()\n",
    "    W_tensor2matrix = W_tensor_rec.reshape(W.shape)  \n",
    "    d,N = V.size()\n",
    "    I = torch.eye(N, device=device)\n",
    "    U_prime = torch.t(U).reshape(n*Hprime*Wprime,-1)\n",
    "    Wstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(V),V))+(alpha+tau)*I), rho*torch.mm(torch.t(V),U_prime)+alpha*W+tau*W_tensor2matrix)\n",
    "    return Wstar\n",
    "\n",
    "def updateWb_CNNorg(U, V, W, alpha, rho):\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(N, device=device)\n",
    "    #_, col_U = U.size()\n",
    "    U_prime = torch.t(U).reshape(n*Hprime*Wprime,-1)\n",
    "    Wstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(V),V))+alpha*I), rho*torch.mm(torch.t(V),U_prime)+alpha*W)\n",
    "    \n",
    "    return Wstar\n",
    "\n",
    "def updateV(U1,U2,W,b,rho,gamma):\n",
    "    _, d = W.size()\n",
    "    I = torch.eye(d, device=device) #Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "    U1 = nn.ReLU()(U1)\n",
    "    _, col_U2 = U2.size()\n",
    "    Vstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(W),W))+gamma*I), rho*torch.mm(torch.t(W),U2-b.repeat(1,col_U2))+gamma*U1)\n",
    "    return Vstar\n",
    "\n",
    "def updateWb_org(U, V, W, b, alpha, rho):\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(d, device=device)\n",
    "    _, col_U = U.size()\n",
    "    Wstar = torch.mm(alpha*W+rho*torch.mm(U-b.repeat(1,col_U),torch.t(V)),torch.inverse(alpha*I+rho*(torch.mm(V,torch.t(V)))))\n",
    "    bstar = 0*(alpha*b+rho*torch.sum(U-torch.mm(Wstar,V), dim=1).reshape(b.size()))/(rho*N+alpha)\n",
    "    return Wstar, bstar\n",
    "\n",
    "def updateWb(U, V, W, b, W_tensor_rec, alpha, rho,tau):\n",
    "    W_tensor_rec = torch.as_tensor(W_tensor_rec,device=device).float()\n",
    "    W_tensor2matrix = W_tensor_rec.reshape(W.shape)\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(d, device=device)\n",
    "    _, col_U = U.size()\n",
    "    Wstar = torch.mm(alpha*W+tau*W_tensor2matrix+rho*torch.mm(U-b.repeat(1,col_U),torch.t(V)),torch.inverse((alpha+tau)*I+rho*(torch.mm(V,torch.t(V)))))\n",
    "    bstar = 0*(alpha*b+rho*torch.sum(U-torch.mm(Wstar,V), dim=1).reshape(b.size()))/(rho*N+alpha)\n",
    "    return Wstar, bstar\n",
    "\n",
    "def relu_prox(a, b, gamma, d, N):\n",
    "    val = torch.empty(d,N, device=device)\n",
    "    x = (a+gamma*b)/(1+gamma)\n",
    "    y = torch.min(b,torch.zeros(d,N, device=device))\n",
    "\n",
    "    val = torch.where(a+gamma*b < 0, y, torch.zeros(d,N, device=device))\n",
    "    val = torch.where(((a+gamma*b >= 0) & (b >=0)) | ((a*(gamma-np.sqrt(gamma*(gamma+1))) <= gamma*b) & (b < 0)), x, val)\n",
    "    val = torch.where((-a <= gamma*b) & (gamma*b <= a*(gamma-np.sqrt(gamma*(gamma+1)))), b, val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af396bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank= 220 tau= 0.2 gamma= 0.5 rho= 0.5 alpha 1\n",
      "Train on 6000 samples, validate on 999 samples\n",
      "Repeatition 1 Epoch 1 / 2000 \n",
      " - time: 9.255205392837524 - sq_loss: 544.1398315429688 - tot_loss: 864.7251199099352 - acc: 0.17566666666666667 - val_acc: 0.18518518518518517\n",
      "Repeatition 1 Epoch 2 / 2000 \n",
      " - time: 7.2020862102508545 - sq_loss: 241.83990478515625 - tot_loss: 494.70988788371903 - acc: 0.31516666666666665 - val_acc: 0.3233233233233233\n",
      "Repeatition 1 Epoch 3 / 2000 \n",
      " - time: 7.251087427139282 - sq_loss: 121.19127655029297 - tot_loss: 297.04525235183536 - acc: 0.4285 - val_acc: 0.4444444444444444\n",
      "Repeatition 1 Epoch 4 / 2000 \n",
      " - time: 7.428819417953491 - sq_loss: 63.44929504394531 - tot_loss: 183.20882971882818 - acc: 0.5221666666666667 - val_acc: 0.5245245245245245\n",
      "Repeatition 1 Epoch 5 / 2000 \n",
      " - time: 7.21894645690918 - sq_loss: 33.76056671142578 - tot_loss: 116.36636759713294 - acc: 0.5845 - val_acc: 0.5765765765765766\n",
      "Repeatition 1 Epoch 6 / 2000 \n",
      " - time: 7.280803918838501 - sq_loss: 18.090293884277344 - tot_loss: 76.32098039090633 - acc: 0.6318333333333334 - val_acc: 0.6236236236236237\n",
      "Repeatition 1 Epoch 7 / 2000 \n",
      " - time: 7.29580020904541 - sq_loss: 9.738421440124512 - tot_loss: 51.72627519220114 - acc: 0.672 - val_acc: 0.6736736736736737\n",
      "Repeatition 1 Epoch 8 / 2000 \n",
      " - time: 7.264985084533691 - sq_loss: 5.266989231109619 - tot_loss: 36.186950175464155 - acc: 0.7025 - val_acc: 0.7127127127127127\n",
      "Repeatition 1 Epoch 9 / 2000 \n",
      " - time: 7.2982337474823 - sq_loss: 2.864912271499634 - tot_loss: 26.07262909114361 - acc: 0.7266666666666667 - val_acc: 0.7297297297297297\n",
      "Repeatition 1 Epoch 10 / 2000 \n",
      " - time: 7.261337757110596 - sq_loss: 1.5695693492889404 - tot_loss: 19.295247414708136 - acc: 0.7468333333333333 - val_acc: 0.7467467467467468\n",
      "Repeatition 1 Epoch 11 / 2000 \n",
      " - time: 7.143670320510864 - sq_loss: 0.8676766157150269 - tot_loss: 14.630386057496072 - acc: 0.7621666666666667 - val_acc: 0.7677677677677678\n",
      "Repeatition 1 Epoch 12 / 2000 \n",
      " - time: 7.283973455429077 - sq_loss: 0.4850265681743622 - tot_loss: 11.342113587632776 - acc: 0.7736666666666666 - val_acc: 0.7877877877877878\n",
      "Repeatition 1 Epoch 13 / 2000 \n",
      " - time: 7.1978371143341064 - sq_loss: 0.2748328447341919 - tot_loss: 8.975487827509642 - acc: 0.785 - val_acc: 0.7967967967967968\n",
      "Repeatition 1 Epoch 14 / 2000 \n",
      " - time: 7.139495134353638 - sq_loss: 0.15830059349536896 - tot_loss: 7.240925091505051 - acc: 0.7965 - val_acc: 0.8048048048048048\n",
      "Repeatition 1 Epoch 15 / 2000 \n",
      " - time: 7.170307874679565 - sq_loss: 0.09297528862953186 - tot_loss: 5.9488429317250855 - acc: 0.8063333333333333 - val_acc: 0.8088088088088088\n",
      "Repeatition 1 Epoch 16 / 2000 \n",
      " - time: 7.17007040977478 - sq_loss: 0.05587267875671387 - tot_loss: 4.971826736815274 - acc: 0.8146666666666667 - val_acc: 0.8158158158158159\n",
      "Repeatition 1 Epoch 17 / 2000 \n",
      " - time: 7.143728017807007 - sq_loss: 0.034474726766347885 - tot_loss: 4.22260523494333 - acc: 0.8196666666666667 - val_acc: 0.8218218218218218\n",
      "Repeatition 1 Epoch 18 / 2000 \n",
      " - time: 7.193563461303711 - sq_loss: 0.02191334404051304 - tot_loss: 3.6401551343500613 - acc: 0.8258333333333333 - val_acc: 0.8258258258258259\n",
      "Repeatition 1 Epoch 19 / 2000 \n",
      " - time: 7.1003711223602295 - sq_loss: 0.01438918337225914 - tot_loss: 3.181245946697891 - acc: 0.8298333333333333 - val_acc: 0.8318318318318318\n",
      "Repeatition 1 Epoch 20 / 2000 \n",
      " - time: 7.18010950088501 - sq_loss: 0.009778620675206184 - tot_loss: 2.8148052929900587 - acc: 0.834 - val_acc: 0.8398398398398398\n",
      "Repeatition 1 Epoch 21 / 2000 \n",
      " - time: 7.187571287155151 - sq_loss: 0.006881803274154663 - tot_loss: 2.5182908397167925 - acc: 0.838 - val_acc: 0.8458458458458459\n",
      "Repeatition 1 Epoch 22 / 2000 \n",
      " - time: 7.319952011108398 - sq_loss: 0.0050118169747292995 - tot_loss: 2.2752190104685726 - acc: 0.8421666666666666 - val_acc: 0.8488488488488488\n",
      "Repeatition 1 Epoch 23 / 2000 \n",
      " - time: 7.201181411743164 - sq_loss: 0.0037693018093705177 - tot_loss: 2.0734429677017037 - acc: 0.8461666666666666 - val_acc: 0.8508508508508509\n",
      "Repeatition 1 Epoch 24 / 2000 \n",
      " - time: 7.159414291381836 - sq_loss: 0.0029192487709224224 - tot_loss: 1.9038884600624444 - acc: 0.8481666666666666 - val_acc: 0.8508508508508509\n",
      "Repeatition 1 Epoch 25 / 2000 \n",
      " - time: 7.191064834594727 - sq_loss: 0.0023203331511467695 - tot_loss: 1.7597457018680869 - acc: 0.8513333333333334 - val_acc: 0.8528528528528528\n",
      "Repeatition 1 Epoch 26 / 2000 \n",
      " - time: 7.218621253967285 - sq_loss: 0.0018861888675019145 - tot_loss: 1.635852032969706 - acc: 0.8548333333333333 - val_acc: 0.8548548548548549\n",
      "Repeatition 1 Epoch 27 / 2000 \n",
      " - time: 7.14516544342041 - sq_loss: 0.0015630180714651942 - tot_loss: 1.528280330845155 - acc: 0.8566666666666667 - val_acc: 0.8578578578578578\n",
      "Repeatition 1 Epoch 28 / 2000 \n",
      " - time: 7.299946308135986 - sq_loss: 0.0013164104893803596 - tot_loss: 1.4340130807366223 - acc: 0.8601666666666666 - val_acc: 0.8608608608608609\n",
      "Repeatition 1 Epoch 29 / 2000 \n",
      " - time: 7.0835959911346436 - sq_loss: 0.0011241370812058449 - tot_loss: 1.3507148743141444 - acc: 0.8626666666666667 - val_acc: 0.8648648648648649\n",
      "Repeatition 1 Epoch 30 / 2000 \n",
      " - time: 7.099579811096191 - sq_loss: 0.0009712825412862003 - tot_loss: 1.2765430915984324 - acc: 0.8646666666666667 - val_acc: 0.8658658658658659\n",
      "Repeatition 1 Epoch 31 / 2000 \n",
      " - time: 7.1524012088775635 - sq_loss: 0.0008477809024043381 - tot_loss: 1.21003967608558 - acc: 0.867 - val_acc: 0.8678678678678678\n",
      "Repeatition 1 Epoch 32 / 2000 \n",
      " - time: 7.302317380905151 - sq_loss: 0.0007465423550456762 - tot_loss: 1.1500433887355028 - acc: 0.8693333333333333 - val_acc: 0.8718718718718719\n",
      "Repeatition 1 Epoch 33 / 2000 \n",
      " - time: 7.157688140869141 - sq_loss: 0.0006625056848861277 - tot_loss: 1.095609211444389 - acc: 0.8721666666666666 - val_acc: 0.8758758758758759\n",
      "Repeatition 1 Epoch 34 / 2000 \n",
      " - time: 7.210861921310425 - sq_loss: 0.0005920423427596688 - tot_loss: 1.0459796490380542 - acc: 0.8741666666666666 - val_acc: 0.8778778778778779\n",
      "Repeatition 1 Epoch 35 / 2000 \n",
      " - time: 7.1110146045684814 - sq_loss: 0.0005323943332768977 - tot_loss: 1.0005334823508747 - acc: 0.8763333333333333 - val_acc: 0.8788788788788788\n",
      "Repeatition 1 Epoch 36 / 2000 \n",
      " - time: 7.208545207977295 - sq_loss: 0.0004815055872313678 - tot_loss: 0.9587545364745893 - acc: 0.8788333333333334 - val_acc: 0.8798798798798799\n",
      "Repeatition 1 Epoch 37 / 2000 \n",
      " - time: 7.153909683227539 - sq_loss: 0.0004377949226181954 - tot_loss: 0.9202022019249853 - acc: 0.8798333333333334 - val_acc: 0.8808808808808809\n",
      "Repeatition 1 Epoch 38 / 2000 \n",
      " - time: 7.16402268409729 - sq_loss: 0.000400031654862687 - tot_loss: 0.8845052500895692 - acc: 0.8821666666666667 - val_acc: 0.8818818818818819\n",
      "Repeatition 1 Epoch 39 / 2000 \n",
      " - time: 7.083839654922485 - sq_loss: 0.00036720026400871575 - tot_loss: 0.8513567582995166 - acc: 0.8843333333333333 - val_acc: 0.8858858858858859\n",
      "Repeatition 1 Epoch 40 / 2000 \n",
      " - time: 7.2044994831085205 - sq_loss: 0.0003385439049452543 - tot_loss: 0.8204924690071493 - acc: 0.8846666666666667 - val_acc: 0.8868868868868869\n",
      "Repeatition 1 Epoch 41 / 2000 \n",
      " - time: 7.16853666305542 - sq_loss: 0.00031338026747107506 - tot_loss: 0.7916839952813461 - acc: 0.8863333333333333 - val_acc: 0.8858858858858859\n",
      "Repeatition 1 Epoch 42 / 2000 \n",
      " - time: 7.255699157714844 - sq_loss: 0.00029119724058546126 - tot_loss: 0.764734253642382 - acc: 0.8873333333333333 - val_acc: 0.8848848848848849\n",
      "Repeatition 1 Epoch 43 / 2000 \n",
      " - time: 7.087270975112915 - sq_loss: 0.000271559227257967 - tot_loss: 0.7394686241401359 - acc: 0.888 - val_acc: 0.8868868868868869\n",
      "Repeatition 1 Epoch 44 / 2000 \n",
      " - time: 7.382331371307373 - sq_loss: 0.00025409070076420903 - tot_loss: 0.7157345836050809 - acc: 0.8888333333333334 - val_acc: 0.8888888888888888\n",
      "Repeatition 1 Epoch 45 / 2000 \n",
      " - time: 7.178106069564819 - sq_loss: 0.00023851636797189713 - tot_loss: 0.6934012215584516 - acc: 0.8901666666666667 - val_acc: 0.8908908908908909\n",
      "Repeatition 1 Epoch 46 / 2000 \n",
      " - time: 7.217419624328613 - sq_loss: 0.00022457716113422066 - tot_loss: 0.6723487493261928 - acc: 0.8923333333333333 - val_acc: 0.8958958958958959\n",
      "Repeatition 1 Epoch 47 / 2000 \n",
      " - time: 7.214518785476685 - sq_loss: 0.00021207547979429364 - tot_loss: 0.6524701419635676 - acc: 0.8926666666666667 - val_acc: 0.8978978978978979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeatition 1 Epoch 48 / 2000 \n",
      " - time: 7.08837628364563 - sq_loss: 0.00020079300156794488 - tot_loss: 0.6336744050437119 - acc: 0.8936666666666667 - val_acc: 0.8998998998998999\n",
      "Repeatition 1 Epoch 49 / 2000 \n",
      " - time: 7.212125062942505 - sq_loss: 0.00019060580234508961 - tot_loss: 0.6158800520555814 - acc: 0.8951666666666667 - val_acc: 0.8998998998998999\n",
      "Repeatition 1 Epoch 50 / 2000 \n",
      " - time: 7.128115177154541 - sq_loss: 0.0001813338021747768 - tot_loss: 0.5990102576906793 - acc: 0.8966666666666666 - val_acc: 0.8998998998998999\n",
      "Repeatition 1 Epoch 51 / 2000 \n",
      " - time: 7.058708429336548 - sq_loss: 0.0001729174837237224 - tot_loss: 0.5829979657166404 - acc: 0.897 - val_acc: 0.8998998998998999\n",
      "Repeatition 1 Epoch 52 / 2000 \n",
      " - time: 7.180062770843506 - sq_loss: 0.00016522457008250058 - tot_loss: 0.5677834129484837 - acc: 0.898 - val_acc: 0.9029029029029029\n",
      "Repeatition 1 Epoch 53 / 2000 \n",
      " - time: 7.138581275939941 - sq_loss: 0.00015818659448996186 - tot_loss: 0.5533077446161769 - acc: 0.8993333333333333 - val_acc: 0.9029029029029029\n",
      "Repeatition 1 Epoch 54 / 2000 \n",
      " - time: 7.298303842544556 - sq_loss: 0.000151717962580733 - tot_loss: 0.5395215951517457 - acc: 0.9001666666666667 - val_acc: 0.9029029029029029\n",
      "Repeatition 1 Epoch 55 / 2000 \n",
      " - time: 7.233452558517456 - sq_loss: 0.0001457710168324411 - tot_loss: 0.5263783206930385 - acc: 0.9006666666666666 - val_acc: 0.9029029029029029\n",
      "Repeatition 1 Epoch 56 / 2000 \n",
      " - time: 7.131845235824585 - sq_loss: 0.00014027071301825345 - tot_loss: 0.5138341444253456 - acc: 0.901 - val_acc: 0.9059059059059059\n",
      "Repeatition 1 Epoch 57 / 2000 \n",
      " - time: 7.168130874633789 - sq_loss: 0.00013518919877242297 - tot_loss: 0.501851672763587 - acc: 0.9016666666666666 - val_acc: 0.9049049049049049\n",
      "Repeatition 1 Epoch 58 / 2000 \n",
      " - time: 8.20415186882019 - sq_loss: 0.00013047161337453872 - tot_loss: 0.49039601649565157 - acc: 0.9026666666666666 - val_acc: 0.9049049049049049\n",
      "Repeatition 1 Epoch 59 / 2000 \n",
      " - time: 7.201476812362671 - sq_loss: 0.0001260759891010821 - tot_loss: 0.47943637702264824 - acc: 0.9035 - val_acc: 0.9059059059059059\n",
      "Repeatition 1 Epoch 60 / 2000 \n",
      " - time: 7.155835390090942 - sq_loss: 0.00012200382479932159 - tot_loss: 0.46894272239587736 - acc: 0.9038333333333334 - val_acc: 0.9059059059059059\n",
      "Repeatition 1 Epoch 61 / 2000 \n",
      " - time: 7.080475807189941 - sq_loss: 0.00011818217899417505 - tot_loss: 0.45888759303343246 - acc: 0.9048333333333334 - val_acc: 0.9059059059059059\n",
      "Repeatition 1 Epoch 62 / 2000 \n",
      " - time: 7.241436719894409 - sq_loss: 0.00011460898531368002 - tot_loss: 0.4492445562136709 - acc: 0.905 - val_acc: 0.9059059059059059\n",
      "Repeatition 1 Epoch 63 / 2000 \n",
      " - time: 7.137510776519775 - sq_loss: 0.00011126891331514344 - tot_loss: 0.4399908718754887 - acc: 0.9065 - val_acc: 0.9069069069069069\n",
      "Repeatition 1 Epoch 64 / 2000 \n",
      " - time: 7.219585180282593 - sq_loss: 0.00010812922118930146 - tot_loss: 0.4311050006872392 - acc: 0.9075 - val_acc: 0.9069069069069069\n",
      "Repeatition 1 Epoch 65 / 2000 \n",
      " - time: 7.140376567840576 - sq_loss: 0.00010516917973291129 - tot_loss: 0.4225642751756823 - acc: 0.908 - val_acc: 0.9079079079079079\n",
      "Repeatition 1 Epoch 66 / 2000 \n",
      " - time: 7.222804307937622 - sq_loss: 0.00010236263915430754 - tot_loss: 0.4143514198745834 - acc: 0.9085 - val_acc: 0.908908908908909\n",
      "Repeatition 1 Epoch 67 / 2000 \n",
      " - time: 7.327863454818726 - sq_loss: 9.972760017262772e-05 - tot_loss: 0.4064508503084653 - acc: 0.9083333333333333 - val_acc: 0.9099099099099099\n",
      "Repeatition 1 Epoch 68 / 2000 \n",
      " - time: 7.195871114730835 - sq_loss: 9.722649701870978e-05 - tot_loss: 0.39884501737542455 - acc: 0.9093333333333333 - val_acc: 0.9099099099099099\n",
      "Repeatition 1 Epoch 69 / 2000 \n",
      " - time: 7.0792999267578125 - sq_loss: 9.485567716183141e-05 - tot_loss: 0.39151867246982874 - acc: 0.91 - val_acc: 0.9099099099099099\n",
      "Repeatition 1 Epoch 70 / 2000 \n",
      " - time: 7.31546688079834 - sq_loss: 9.260953811462969e-05 - tot_loss: 0.3844566745770862 - acc: 0.9105 - val_acc: 0.9109109109109109\n",
      "Repeatition 1 Epoch 71 / 2000 \n",
      " - time: 7.0737693309783936 - sq_loss: 9.047046478372067e-05 - tot_loss: 0.3776446492964169 - acc: 0.9113333333333333 - val_acc: 0.9109109109109109\n",
      "Repeatition 1 Epoch 72 / 2000 \n",
      " - time: 7.089439868927002 - sq_loss: 8.843018440529704e-05 - tot_loss: 0.37107130759395657 - acc: 0.9118333333333334 - val_acc: 0.9099099099099099\n",
      "Repeatition 1 Epoch 73 / 2000 \n",
      " - time: 7.1476030349731445 - sq_loss: 8.648568473290652e-05 - tot_loss: 0.3647253506380366 - acc: 0.912 - val_acc: 0.9099099099099099\n",
      "Repeatition 1 Epoch 74 / 2000 \n",
      " - time: 7.102703809738159 - sq_loss: 8.463128324365243e-05 - tot_loss: 0.35859656834945786 - acc: 0.9123333333333333 - val_acc: 0.9099099099099099\n",
      "Repeatition 1 Epoch 75 / 2000 \n",
      " - time: 7.2057623863220215 - sq_loss: 8.285154035547748e-05 - tot_loss: 0.352674493608356 - acc: 0.9131666666666667 - val_acc: 0.9099099099099099\n",
      "Repeatition 1 Epoch 76 / 2000 \n",
      " - time: 7.153399467468262 - sq_loss: 8.115041418932378e-05 - tot_loss: 0.3469488442235161 - acc: 0.9136666666666666 - val_acc: 0.9109109109109109\n",
      "Repeatition 1 Epoch 77 / 2000 \n",
      " - time: 7.150627136230469 - sq_loss: 7.951560110086575e-05 - tot_loss: 0.341409724195546 - acc: 0.9138333333333334 - val_acc: 0.9109109109109109\n",
      "Repeatition 1 Epoch 78 / 2000 \n",
      " - time: 7.1657304763793945 - sq_loss: 7.794697012286633e-05 - tot_loss: 0.3360486990568461 - acc: 0.915 - val_acc: 0.9119119119119119\n",
      "Repeatition 1 Epoch 79 / 2000 \n",
      " - time: 7.110985040664673 - sq_loss: 7.643762364750728e-05 - tot_loss: 0.3308577521689585 - acc: 0.9156666666666666 - val_acc: 0.9119119119119119\n",
      "Repeatition 1 Epoch 80 / 2000 \n",
      " - time: 7.102790832519531 - sq_loss: 7.498924242099747e-05 - tot_loss: 0.32582889457553393 - acc: 0.9156666666666666 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 81 / 2000 \n",
      " - time: 7.107628345489502 - sq_loss: 7.360253221122548e-05 - tot_loss: 0.32095618438470414 - acc: 0.9165 - val_acc: 0.9129129129129129\n",
      "Repeatition 1 Epoch 82 / 2000 \n",
      " - time: 7.288114309310913 - sq_loss: 7.225542503874749e-05 - tot_loss: 0.31623367984720974 - acc: 0.9176666666666666 - val_acc: 0.913913913913914\n",
      "Repeatition 1 Epoch 83 / 2000 \n",
      " - time: 7.194970607757568 - sq_loss: 7.096253102645278e-05 - tot_loss: 0.3116543486365117 - acc: 0.9181666666666667 - val_acc: 0.913913913913914\n",
      "Repeatition 1 Epoch 84 / 2000 \n",
      " - time: 7.103119134902954 - sq_loss: 6.972193659748882e-05 - tot_loss: 0.30721180538239423 - acc: 0.9185 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 85 / 2000 \n",
      " - time: 7.384755373001099 - sq_loss: 6.852881779195741e-05 - tot_loss: 0.3029000752445427 - acc: 0.919 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 86 / 2000 \n",
      " - time: 7.159776210784912 - sq_loss: 6.737343937857077e-05 - tot_loss: 0.29871325795975284 - acc: 0.9196666666666666 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 87 / 2000 \n",
      " - time: 7.228738784790039 - sq_loss: 6.62513921270147e-05 - tot_loss: 0.29464625284890644 - acc: 0.92 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 88 / 2000 \n",
      " - time: 7.10174298286438 - sq_loss: 6.51651862426661e-05 - tot_loss: 0.29069454329583094 - acc: 0.9208333333333333 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 89 / 2000 \n",
      " - time: 7.1503005027771 - sq_loss: 6.410765490727499e-05 - tot_loss: 0.286853724445973 - acc: 0.9213333333333333 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 90 / 2000 \n",
      " - time: 7.151879072189331 - sq_loss: 6.309179298114032e-05 - tot_loss: 0.2831192324258154 - acc: 0.922 - val_acc: 0.9169169169169169\n",
      "Repeatition 1 Epoch 91 / 2000 \n",
      " - time: 7.13913631439209 - sq_loss: 6.210870196809992e-05 - tot_loss: 0.2794866228083265 - acc: 0.9228333333333333 - val_acc: 0.9179179179179179\n",
      "Repeatition 1 Epoch 92 / 2000 \n",
      " - time: 7.153834342956543 - sq_loss: 6.114539428381249e-05 - tot_loss: 0.275951173818612 - acc: 0.9233333333333333 - val_acc: 0.9179179179179179\n",
      "Repeatition 1 Epoch 93 / 2000 \n",
      " - time: 7.244708299636841 - sq_loss: 6.021541776135564e-05 - tot_loss: 0.27250931109883825 - acc: 0.9236666666666666 - val_acc: 0.918918918918919\n",
      "Repeatition 1 Epoch 94 / 2000 \n",
      " - time: 7.163817882537842 - sq_loss: 5.931837949901819e-05 - tot_loss: 0.26915762534481474 - acc: 0.9238333333333333 - val_acc: 0.91991991991992\n",
      "Repeatition 1 Epoch 95 / 2000 \n",
      " - time: 7.30439305305481 - sq_loss: 5.8446959883440286e-05 - tot_loss: 0.2658926083051483 - acc: 0.9245 - val_acc: 0.9219219219219219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeatition 1 Epoch 96 / 2000 \n",
      " - time: 7.087375164031982 - sq_loss: 5.759543273597956e-05 - tot_loss: 0.26271153535053604 - acc: 0.9248333333333333 - val_acc: 0.9219219219219219\n",
      "Repeatition 1 Epoch 97 / 2000 \n",
      " - time: 7.138582468032837 - sq_loss: 5.676204091287218e-05 - tot_loss: 0.2596113440427871 - acc: 0.9248333333333333 - val_acc: 0.9219219219219219\n",
      "Repeatition 1 Epoch 98 / 2000 \n",
      " - time: 7.169498682022095 - sq_loss: 5.59559885005001e-05 - tot_loss: 0.25658906243406815 - acc: 0.9255 - val_acc: 0.9219219219219219\n",
      "Repeatition 1 Epoch 99 / 2000 \n",
      " - time: 7.253963947296143 - sq_loss: 5.51670091226697e-05 - tot_loss: 0.2536419900046894 - acc: 0.9261666666666667 - val_acc: 0.9219219219219219\n",
      "Repeatition 1 Epoch 100 / 2000 \n",
      " - time: 7.120121479034424 - sq_loss: 5.440623135655187e-05 - tot_loss: 0.2507675805878534 - acc: 0.9265 - val_acc: 0.9219219219219219\n",
      "Repeatition 1 Epoch 101 / 2000 \n",
      " - time: 7.1095497608184814 - sq_loss: 5.365602919482626e-05 - tot_loss: 0.2479629892426601 - acc: 0.9275 - val_acc: 0.9219219219219219\n",
      "Repeatition 1 Epoch 102 / 2000 \n",
      " - time: 7.068374395370483 - sq_loss: 5.2929586672689766e-05 - tot_loss: 0.24522593499423237 - acc: 0.928 - val_acc: 0.9219219219219219\n",
      "Repeatition 1 Epoch 103 / 2000 \n",
      " - time: 7.073823928833008 - sq_loss: 5.221590618020855e-05 - tot_loss: 0.24255445396047434 - acc: 0.9285 - val_acc: 0.9219219219219219\n",
      "Repeatition 1 Epoch 104 / 2000 \n",
      " - time: 7.159676551818848 - sq_loss: 5.1526589231798425e-05 - tot_loss: 0.23994584220417892 - acc: 0.9288333333333333 - val_acc: 0.9229229229229229\n",
      "Repeatition 1 Epoch 105 / 2000 \n",
      " - time: 7.438992023468018 - sq_loss: 5.085955126560293e-05 - tot_loss: 0.2373978933224862 - acc: 0.9293333333333333 - val_acc: 0.9229229229229229\n",
      "Repeatition 1 Epoch 106 / 2000 \n",
      " - time: 7.233577013015747 - sq_loss: 5.02015755046159e-05 - tot_loss: 0.23490823236643335 - acc: 0.9296666666666666 - val_acc: 0.9229229229229229\n",
      "Repeatition 1 Epoch 107 / 2000 \n",
      " - time: 7.15436315536499 - sq_loss: 4.955796975991689e-05 - tot_loss: 0.232474861866649 - acc: 0.9298333333333333 - val_acc: 0.923923923923924\n",
      "Repeatition 1 Epoch 108 / 2000 \n",
      " - time: 7.057687520980835 - sq_loss: 4.893483128398657e-05 - tot_loss: 0.23009569736314006 - acc: 0.9306666666666666 - val_acc: 0.923923923923924\n",
      "Repeatition 1 Epoch 109 / 2000 \n",
      " - time: 7.176886320114136 - sq_loss: 4.833031562156975e-05 - tot_loss: 0.22776906515937298 - acc: 0.9305 - val_acc: 0.923923923923924\n",
      "Repeatition 1 Epoch 110 / 2000 \n",
      " - time: 7.0546064376831055 - sq_loss: 4.77372195746284e-05 - tot_loss: 0.22549350790141034 - acc: 0.931 - val_acc: 0.923923923923924\n",
      "Repeatition 1 Epoch 111 / 2000 \n",
      " - time: 7.243109941482544 - sq_loss: 4.7158195229712874e-05 - tot_loss: 0.22326756949041737 - acc: 0.9311666666666667 - val_acc: 0.923923923923924\n",
      "Repeatition 1 Epoch 112 / 2000 \n",
      " - time: 7.033118724822998 - sq_loss: 4.658654870581813e-05 - tot_loss: 0.22108959002725898 - acc: 0.9316666666666666 - val_acc: 0.923923923923924\n",
      "Repeatition 1 Epoch 113 / 2000 \n",
      " - time: 7.177878379821777 - sq_loss: 4.603567867889069e-05 - tot_loss: 0.21895824395978708 - acc: 0.9323333333333333 - val_acc: 0.923923923923924\n",
      "Repeatition 1 Epoch 114 / 2000 \n",
      " - time: 7.060297012329102 - sq_loss: 4.54962755611632e-05 - tot_loss: 0.21687192301396863 - acc: 0.9325 - val_acc: 0.924924924924925\n",
      "Repeatition 1 Epoch 115 / 2000 \n",
      " - time: 7.233679533004761 - sq_loss: 4.496132169151679e-05 - tot_loss: 0.21482850002212217 - acc: 0.9326666666666666 - val_acc: 0.924924924924925\n",
      "Repeatition 1 Epoch 116 / 2000 \n",
      " - time: 7.082681655883789 - sq_loss: 4.443844227353111e-05 - tot_loss: 0.21282647495827403 - acc: 0.9331666666666667 - val_acc: 0.9259259259259259\n",
      "Repeatition 1 Epoch 117 / 2000 \n",
      " - time: 7.10512375831604 - sq_loss: 4.3923737393924966e-05 - tot_loss: 0.2108654700605257 - acc: 0.9331666666666667 - val_acc: 0.9259259259259259\n",
      "Repeatition 1 Epoch 118 / 2000 \n",
      " - time: 7.185765266418457 - sq_loss: 4.3426305637694895e-05 - tot_loss: 0.20894421105622316 - acc: 0.9333333333333333 - val_acc: 0.9259259259259259\n",
      "Repeatition 1 Epoch 119 / 2000 \n",
      " - time: 7.08253812789917 - sq_loss: 4.293741221772507e-05 - tot_loss: 0.20706144476280317 - acc: 0.9336666666666666 - val_acc: 0.9259259259259259\n",
      "Repeatition 1 Epoch 120 / 2000 \n",
      " - time: 7.29790735244751 - sq_loss: 4.245777381584048e-05 - tot_loss: 0.20521600853826386 - acc: 0.934 - val_acc: 0.9259259259259259\n",
      "Repeatition 1 Epoch 121 / 2000 \n",
      " - time: 7.103992462158203 - sq_loss: 4.198692840873264e-05 - tot_loss: 0.20340654505052952 - acc: 0.934 - val_acc: 0.9259259259259259\n",
      "Repeatition 1 Epoch 122 / 2000 \n",
      " - time: 7.132603645324707 - sq_loss: 4.152225301368162e-05 - tot_loss: 0.2016320974289556 - acc: 0.9343333333333333 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 123 / 2000 \n",
      " - time: 7.185715436935425 - sq_loss: 4.1080289520323277e-05 - tot_loss: 0.19989202423894312 - acc: 0.9343333333333333 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 124 / 2000 \n",
      " - time: 7.052177906036377 - sq_loss: 4.064162203576416e-05 - tot_loss: 0.19818471429462078 - acc: 0.9345 - val_acc: 0.9259259259259259\n",
      "Repeatition 1 Epoch 125 / 2000 \n",
      " - time: 7.222760915756226 - sq_loss: 4.021433414891362e-05 - tot_loss: 0.19650936382531653 - acc: 0.9346666666666666 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 126 / 2000 \n",
      " - time: 7.300732612609863 - sq_loss: 3.979235771112144e-05 - tot_loss: 0.1948649559635669 - acc: 0.9348333333333333 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 127 / 2000 \n",
      " - time: 7.160444021224976 - sq_loss: 3.9380378439091146e-05 - tot_loss: 0.19325060946866868 - acc: 0.9355 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 128 / 2000 \n",
      " - time: 7.10103702545166 - sq_loss: 3.897460919688456e-05 - tot_loss: 0.19166580404198613 - acc: 0.9355 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 129 / 2000 \n",
      " - time: 7.126591444015503 - sq_loss: 3.858330819639377e-05 - tot_loss: 0.19010969136070344 - acc: 0.9356666666666666 - val_acc: 0.9279279279279279\n",
      "Repeatition 1 Epoch 130 / 2000 \n",
      " - time: 7.100462913513184 - sq_loss: 3.819218909484334e-05 - tot_loss: 0.18858160878080527 - acc: 0.9361666666666667 - val_acc: 0.9279279279279279\n",
      "Repeatition 1 Epoch 131 / 2000 \n",
      " - time: 7.098159313201904 - sq_loss: 3.780570477829315e-05 - tot_loss: 0.18708061851284583 - acc: 0.9366666666666666 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 132 / 2000 \n",
      " - time: 7.195425748825073 - sq_loss: 3.7427897041197866e-05 - tot_loss: 0.18560585926898057 - acc: 0.9366666666666666 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 133 / 2000 \n",
      " - time: 7.046712160110474 - sq_loss: 3.705888957483694e-05 - tot_loss: 0.18415623689797941 - acc: 0.937 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 134 / 2000 \n",
      " - time: 7.297334432601929 - sq_loss: 3.670360820251517e-05 - tot_loss: 0.18273127517823012 - acc: 0.9371666666666667 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 135 / 2000 \n",
      " - time: 7.091394424438477 - sq_loss: 3.634377571870573e-05 - tot_loss: 0.18133054620775513 - acc: 0.9373333333333334 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 136 / 2000 \n",
      " - time: 7.0925023555755615 - sq_loss: 3.599181945901364e-05 - tot_loss: 0.17995318485918688 - acc: 0.9376666666666666 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 137 / 2000 \n",
      " - time: 7.094573259353638 - sq_loss: 3.564693543012254e-05 - tot_loss: 0.17859878980889335 - acc: 0.9381666666666667 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 138 / 2000 \n",
      " - time: 7.216975450515747 - sq_loss: 3.530787580530159e-05 - tot_loss: 0.17726702677973663 - acc: 0.9381666666666667 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 139 / 2000 \n",
      " - time: 7.036854267120361 - sq_loss: 3.4979646443389356e-05 - tot_loss: 0.17595716837386136 - acc: 0.9385 - val_acc: 0.9269269269269269\n",
      "Repeatition 1 Epoch 140 / 2000 \n",
      " - time: 7.221654415130615 - sq_loss: 3.465503687039018e-05 - tot_loss: 0.17466855950915486 - acc: 0.9388333333333333 - val_acc: 0.9279279279279279\n",
      "Repeatition 1 Epoch 141 / 2000 \n",
      " - time: 7.116703987121582 - sq_loss: 3.433541496633552e-05 - tot_loss: 0.17340101301742833 - acc: 0.9391666666666667 - val_acc: 0.9279279279279279\n",
      "Repeatition 1 Epoch 142 / 2000 \n",
      " - time: 7.0451719760894775 - sq_loss: 3.402091169846244e-05 - tot_loss: 0.17215380019697477 - acc: 0.9393333333333334 - val_acc: 0.9279279279279279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeatition 1 Epoch 143 / 2000 \n",
      " - time: 7.112608432769775 - sq_loss: 3.370768536115065e-05 - tot_loss: 0.1709261659590993 - acc: 0.9393333333333334 - val_acc: 0.9279279279279279\n",
      "Repeatition 1 Epoch 144 / 2000 \n",
      " - time: 7.073323488235474 - sq_loss: 3.33984280587174e-05 - tot_loss: 0.16971790543902898 - acc: 0.9393333333333334 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 145 / 2000 \n",
      " - time: 7.129088640213013 - sq_loss: 3.3102773159043863e-05 - tot_loss: 0.16852850625946303 - acc: 0.9396666666666667 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 146 / 2000 \n",
      " - time: 7.084439992904663 - sq_loss: 3.281085810158402e-05 - tot_loss: 0.1673575388034806 - acc: 0.9401666666666667 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 147 / 2000 \n",
      " - time: 7.24543833732605 - sq_loss: 3.252211899962276e-05 - tot_loss: 0.1662043207339593 - acc: 0.9403333333333334 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 148 / 2000 \n",
      " - time: 7.115640878677368 - sq_loss: 3.2239935535471886e-05 - tot_loss: 0.1650681744751637 - acc: 0.9403333333333334 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 149 / 2000 \n",
      " - time: 7.171430826187134 - sq_loss: 3.1959909392753616e-05 - tot_loss: 0.16394878791688824 - acc: 0.9411666666666667 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 150 / 2000 \n",
      " - time: 7.261091947555542 - sq_loss: 3.168494731653482e-05 - tot_loss: 0.1628460661202553 - acc: 0.9415 - val_acc: 0.9279279279279279\n",
      "Repeatition 1 Epoch 151 / 2000 \n",
      " - time: 7.1602935791015625 - sq_loss: 3.141265915473923e-05 - tot_loss: 0.16175974009966013 - acc: 0.9415 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 152 / 2000 \n",
      " - time: 7.181386709213257 - sq_loss: 3.1146137189352885e-05 - tot_loss: 0.16068943598002078 - acc: 0.9416666666666667 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 153 / 2000 \n",
      " - time: 7.057239055633545 - sq_loss: 3.088179437327199e-05 - tot_loss: 0.1596348369428597 - acc: 0.9421666666666667 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 154 / 2000 \n",
      " - time: 7.083636045455933 - sq_loss: 3.0625247745774686e-05 - tot_loss: 0.15859549014712684 - acc: 0.9425 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 155 / 2000 \n",
      " - time: 7.131853103637695 - sq_loss: 3.0373203117051162e-05 - tot_loss: 0.15757106786659278 - acc: 0.9426666666666667 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 156 / 2000 \n",
      " - time: 7.219757795333862 - sq_loss: 3.012363777088467e-05 - tot_loss: 0.1565614500348602 - acc: 0.9428333333333333 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 157 / 2000 \n",
      " - time: 7.327881813049316 - sq_loss: 2.9877244742237963e-05 - tot_loss: 0.15556596730348246 - acc: 0.9428333333333333 - val_acc: 0.928928928928929\n",
      "Repeatition 1 Epoch 158 / 2000 \n",
      " - time: 7.214489221572876 - sq_loss: 2.963479346362874e-05 - tot_loss: 0.15458409503771692 - acc: 0.9428333333333333 - val_acc: 0.9309309309309309\n",
      "Repeatition 1 Epoch 159 / 2000 \n",
      " - time: 7.388364553451538 - sq_loss: 2.9397218895610422e-05 - tot_loss: 0.15361567914515034 - acc: 0.9433333333333334 - val_acc: 0.9319319319319319\n",
      "Repeatition 1 Epoch 160 / 2000 \n",
      " - time: 7.2200822830200195 - sq_loss: 2.9161214115447365e-05 - tot_loss: 0.15266044639975007 - acc: 0.9435 - val_acc: 0.9319319319319319\n",
      "Repeatition 1 Epoch 161 / 2000 \n",
      " - time: 7.186933279037476 - sq_loss: 2.8927386665600352e-05 - tot_loss: 0.15171833837339363 - acc: 0.9443333333333334 - val_acc: 0.9329329329329329\n",
      "Repeatition 1 Epoch 162 / 2000 \n",
      " - time: 7.019120931625366 - sq_loss: 2.8703067073365673e-05 - tot_loss: 0.15078869926583138 - acc: 0.9446666666666667 - val_acc: 0.9329329329329329\n",
      "Repeatition 1 Epoch 163 / 2000 \n",
      " - time: 7.184483766555786 - sq_loss: 2.847659015969839e-05 - tot_loss: 0.1498715547215397 - acc: 0.945 - val_acc: 0.933933933933934\n",
      "Repeatition 1 Epoch 164 / 2000 \n",
      " - time: 7.173379182815552 - sq_loss: 2.8256186851649545e-05 - tot_loss: 0.148966846217445 - acc: 0.9453333333333334 - val_acc: 0.933933933933934\n",
      "Repeatition 1 Epoch 165 / 2000 \n",
      " - time: 7.193960428237915 - sq_loss: 2.804344148898963e-05 - tot_loss: 0.14807440800977928 - acc: 0.9453333333333334 - val_acc: 0.934934934934935\n",
      "Repeatition 1 Epoch 166 / 2000 \n",
      " - time: 7.145288944244385 - sq_loss: 2.7829657483380288e-05 - tot_loss: 0.14719391202088447 - acc: 0.9456666666666667 - val_acc: 0.934934934934935\n",
      "Repeatition 1 Epoch 167 / 2000 \n",
      " - time: 7.236388206481934 - sq_loss: 2.7611675250227563e-05 - tot_loss: 0.1463249055432243 - acc: 0.9456666666666667 - val_acc: 0.934934934934935\n",
      "Repeatition 1 Epoch 168 / 2000 \n",
      " - time: 7.272615194320679 - sq_loss: 2.740196396189276e-05 - tot_loss: 0.14546714623174922 - acc: 0.946 - val_acc: 0.934934934934935\n",
      "Repeatition 1 Epoch 169 / 2000 \n",
      " - time: 7.350941896438599 - sq_loss: 2.719357689784374e-05 - tot_loss: 0.14462022299667296 - acc: 0.946 - val_acc: 0.934934934934935\n",
      "Repeatition 1 Epoch 170 / 2000 \n",
      " - time: 7.152987241744995 - sq_loss: 2.698464049899485e-05 - tot_loss: 0.14378387367869436 - acc: 0.9461666666666667 - val_acc: 0.934934934934935\n",
      "Repeatition 1 Epoch 171 / 2000 \n",
      " - time: 7.346774101257324 - sq_loss: 2.6786115995491855e-05 - tot_loss: 0.14295799664505468 - acc: 0.9463333333333334 - val_acc: 0.934934934934935\n",
      "Repeatition 1 Epoch 172 / 2000 \n",
      " - time: 7.203718185424805 - sq_loss: 2.6590661946102045e-05 - tot_loss: 0.14214239405228 - acc: 0.9465 - val_acc: 0.934934934934935\n",
      "Repeatition 1 Epoch 173 / 2000 \n",
      " - time: 7.118250131607056 - sq_loss: 2.6393885491415858e-05 - tot_loss: 0.14133698695513885 - acc: 0.9466666666666667 - val_acc: 0.9359359359359359\n",
      "Repeatition 1 Epoch 174 / 2000 \n",
      " - time: 7.052847862243652 - sq_loss: 2.6201749278698117e-05 - tot_loss: 0.14054151052550878 - acc: 0.9468333333333333 - val_acc: 0.9359359359359359\n",
      "Repeatition 1 Epoch 175 / 2000 \n",
      " - time: 7.173175096511841 - sq_loss: 2.6015113689936697e-05 - tot_loss: 0.139755739478278 - acc: 0.9468333333333333 - val_acc: 0.9369369369369369\n",
      "Repeatition 1 Epoch 176 / 2000 \n",
      " - time: 7.075860500335693 - sq_loss: 2.5826364435488358e-05 - tot_loss: 0.13897966911099502 - acc: 0.947 - val_acc: 0.9369369369369369\n",
      "Repeatition 1 Epoch 177 / 2000 \n",
      " - time: 7.1891701221466064 - sq_loss: 2.5640274543548003e-05 - tot_loss: 0.1382129730707675 - acc: 0.9473333333333334 - val_acc: 0.9369369369369369\n",
      "Repeatition 1 Epoch 178 / 2000 \n",
      " - time: 7.207123517990112 - sq_loss: 2.546281939430628e-05 - tot_loss: 0.13745521257842483 - acc: 0.9475 - val_acc: 0.9369369369369369\n",
      "Repeatition 1 Epoch 179 / 2000 \n",
      " - time: 7.153796911239624 - sq_loss: 2.5286175514338538e-05 - tot_loss: 0.13670650309868504 - acc: 0.9476666666666667 - val_acc: 0.9369369369369369\n",
      "Repeatition 1 Epoch 180 / 2000 \n",
      " - time: 7.149302959442139 - sq_loss: 2.5114528398262337e-05 - tot_loss: 0.13596684807343992 - acc: 0.9478333333333333 - val_acc: 0.9369369369369369\n",
      "Repeatition 1 Epoch 181 / 2000 \n",
      " - time: 7.103414058685303 - sq_loss: 2.493976899131667e-05 - tot_loss: 0.13523602454042702 - acc: 0.948 - val_acc: 0.9369369369369369\n",
      "Repeatition 1 Epoch 182 / 2000 \n",
      " - time: 7.09224796295166 - sq_loss: 2.476720510458108e-05 - tot_loss: 0.13451361457082384 - acc: 0.9481666666666667 - val_acc: 0.9379379379379379\n",
      "Repeatition 1 Epoch 183 / 2000 \n",
      " - time: 7.215668439865112 - sq_loss: 2.459486131556332e-05 - tot_loss: 0.13379950051312337 - acc: 0.9485 - val_acc: 0.93993993993994\n",
      "Repeatition 1 Epoch 184 / 2000 \n",
      " - time: 7.180393218994141 - sq_loss: 2.4429991754004732e-05 - tot_loss: 0.13309371811992604 - acc: 0.949 - val_acc: 0.93993993993994\n",
      "Repeatition 1 Epoch 185 / 2000 \n",
      " - time: 7.147577524185181 - sq_loss: 2.4264969397336245e-05 - tot_loss: 0.13239613640471362 - acc: 0.949 - val_acc: 0.93993993993994\n",
      "Repeatition 1 Epoch 186 / 2000 \n",
      " - time: 7.028013467788696 - sq_loss: 2.4101027520373464e-05 - tot_loss: 0.13170661010808546 - acc: 0.9491666666666667 - val_acc: 0.93993993993994\n",
      "Repeatition 1 Epoch 187 / 2000 \n",
      " - time: 7.106622934341431 - sq_loss: 2.393892646068707e-05 - tot_loss: 0.13102516550716245 - acc: 0.9491666666666667 - val_acc: 0.93993993993994\n",
      "Repeatition 1 Epoch 188 / 2000 \n",
      " - time: 7.168255090713501 - sq_loss: 2.3776850866852328e-05 - tot_loss: 0.13035134925230524 - acc: 0.9491666666666667 - val_acc: 0.9409409409409409\n",
      "Repeatition 1 Epoch 189 / 2000 \n",
      " - time: 7.117809534072876 - sq_loss: 2.3621601940249093e-05 - tot_loss: 0.12968478406146458 - acc: 0.9493333333333334 - val_acc: 0.9409409409409409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeatition 1 Epoch 190 / 2000 \n",
      " - time: 7.417524099349976 - sq_loss: 2.3468763174605556e-05 - tot_loss: 0.1290255043833895 - acc: 0.9493333333333334 - val_acc: 0.9409409409409409\n",
      "Repeatition 1 Epoch 191 / 2000 \n",
      " - time: 7.227215766906738 - sq_loss: 2.331610448891297e-05 - tot_loss: 0.1283736170778866 - acc: 0.95 - val_acc: 0.9409409409409409\n",
      "Repeatition 1 Epoch 192 / 2000 \n",
      " - time: 7.131741762161255 - sq_loss: 2.3165548554970883e-05 - tot_loss: 0.12772895880298166 - acc: 0.9503333333333334 - val_acc: 0.9409409409409409\n",
      "Repeatition 1 Epoch 193 / 2000 \n",
      " - time: 7.235118865966797 - sq_loss: 2.301474341948051e-05 - tot_loss: 0.12709136379016855 - acc: 0.9503333333333334 - val_acc: 0.9409409409409409\n",
      "Repeatition 1 Epoch 194 / 2000 \n",
      " - time: 7.053586721420288 - sq_loss: 2.2869795429869555e-05 - tot_loss: 0.12646055106852144 - acc: 0.9506666666666667 - val_acc: 0.9409409409409409\n",
      "Repeatition 1 Epoch 195 / 2000 \n",
      " - time: 7.085097789764404 - sq_loss: 2.2725276721757837e-05 - tot_loss: 0.1258364713870833 - acc: 0.9506666666666667 - val_acc: 0.9409409409409409\n",
      "Repeatition 1 Epoch 196 / 2000 \n",
      " - time: 7.090893983840942 - sq_loss: 2.2577523850486614e-05 - tot_loss: 0.12521927010857326 - acc: 0.9508333333333333 - val_acc: 0.9409409409409409\n",
      "Repeatition 1 Epoch 197 / 2000 \n",
      " - time: 7.234342336654663 - sq_loss: 2.2435855498770252e-05 - tot_loss: 0.12460863238302408 - acc: 0.9508333333333333 - val_acc: 0.9419419419419419\n",
      "Repeatition 1 Epoch 198 / 2000 \n",
      " - time: 7.112049341201782 - sq_loss: 2.2297514078672975e-05 - tot_loss: 0.12400433655275266 - acc: 0.951 - val_acc: 0.9429429429429429\n",
      "Repeatition 1 Epoch 199 / 2000 \n",
      " - time: 7.129740476608276 - sq_loss: 2.2159503714647144e-05 - tot_loss: 0.12340648533572675 - acc: 0.9513333333333334 - val_acc: 0.9429429429429429\n",
      "Repeatition 1 Epoch 200 / 2000 \n",
      " - time: 7.253085136413574 - sq_loss: 2.2021946278982796e-05 - tot_loss: 0.1228149143007613 - acc: 0.9518333333333333 - val_acc: 0.9429429429429429\n",
      "Repeatition 1 Epoch 201 / 2000 \n",
      " - time: 7.042263746261597 - sq_loss: 2.188787766499445e-05 - tot_loss: 0.12222927177208476 - acc: 0.9518333333333333 - val_acc: 0.9429429429429429\n",
      "Repeatition 1 Epoch 202 / 2000 \n",
      " - time: 7.206964492797852 - sq_loss: 2.1755955458502285e-05 - tot_loss: 0.12164957717868674 - acc: 0.9518333333333333 - val_acc: 0.9429429429429429\n",
      "Repeatition 1 Epoch 203 / 2000 \n",
      " - time: 7.126576662063599 - sq_loss: 2.162262171623297e-05 - tot_loss: 0.12107603227559594 - acc: 0.952 - val_acc: 0.9429429429429429\n",
      "Repeatition 1 Epoch 204 / 2000 \n",
      " - time: 7.192411422729492 - sq_loss: 2.149093234038446e-05 - tot_loss: 0.12050855300221883 - acc: 0.952 - val_acc: 0.9429429429429429\n",
      "Repeatition 1 Epoch 205 / 2000 \n",
      " - time: 7.147354602813721 - sq_loss: 2.1357433070079423e-05 - tot_loss: 0.11994693950582587 - acc: 0.9521666666666667 - val_acc: 0.9429429429429429\n",
      "Repeatition 1 Epoch 206 / 2000 \n",
      " - time: 7.103045463562012 - sq_loss: 2.122848673025146e-05 - tot_loss: 0.1193909342677216 - acc: 0.9523333333333334 - val_acc: 0.9429429429429429\n",
      "Repeatition 1 Epoch 207 / 2000 \n",
      " - time: 7.1246116161346436 - sq_loss: 2.110330569848884e-05 - tot_loss: 0.11884041391640494 - acc: 0.9526666666666667 - val_acc: 0.943943943943944\n",
      "Repeatition 1 Epoch 208 / 2000 \n",
      " - time: 7.132389783859253 - sq_loss: 2.098144614137709e-05 - tot_loss: 0.11829544441134203 - acc: 0.9528333333333333 - val_acc: 0.943943943943944\n",
      "Repeatition 1 Epoch 209 / 2000 \n",
      " - time: 7.11537766456604 - sq_loss: 2.0857523850281723e-05 - tot_loss: 0.11775586535841286 - acc: 0.953 - val_acc: 0.943943943943944\n",
      "Repeatition 1 Epoch 210 / 2000 \n",
      " - time: 7.077473878860474 - sq_loss: 2.0736593796755187e-05 - tot_loss: 0.11722165925275477 - acc: 0.9531666666666667 - val_acc: 0.943943943943944\n",
      "Repeatition 1 Epoch 211 / 2000 \n",
      " - time: 7.183284282684326 - sq_loss: 2.061361010419205e-05 - tot_loss: 0.11669278724730248 - acc: 0.9536666666666667 - val_acc: 0.943943943943944\n",
      "Repeatition 1 Epoch 212 / 2000 \n",
      " - time: 7.1146111488342285 - sq_loss: 2.0494506316026673e-05 - tot_loss: 0.11616910964148701 - acc: 0.9538333333333333 - val_acc: 0.943943943943944\n",
      "Repeatition 1 Epoch 213 / 2000 \n",
      " - time: 7.1990907192230225 - sq_loss: 2.0375888198032044e-05 - tot_loss: 0.11565054293132562 - acc: 0.9538333333333333 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 214 / 2000 \n",
      " - time: 7.10543417930603 - sq_loss: 2.0258574295439757e-05 - tot_loss: 0.11513705491852305 - acc: 0.954 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 215 / 2000 \n",
      " - time: 7.123094081878662 - sq_loss: 2.014218807744328e-05 - tot_loss: 0.11462851590949867 - acc: 0.954 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 216 / 2000 \n",
      " - time: 7.144647121429443 - sq_loss: 2.0029270672239363e-05 - tot_loss: 0.1141248890227871 - acc: 0.9541666666666667 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 217 / 2000 \n",
      " - time: 7.177513837814331 - sq_loss: 1.9914032236556523e-05 - tot_loss: 0.11362587261501177 - acc: 0.9545 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 218 / 2000 \n",
      " - time: 7.06353759765625 - sq_loss: 1.980207161977887e-05 - tot_loss: 0.1131315358739812 - acc: 0.9546666666666667 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 219 / 2000 \n",
      " - time: 7.179465293884277 - sq_loss: 1.9690733097377233e-05 - tot_loss: 0.11264195674993971 - acc: 0.9548333333333333 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 220 / 2000 \n",
      " - time: 7.160076141357422 - sq_loss: 1.957989479706157e-05 - tot_loss: 0.1121570560415421 - acc: 0.9548333333333333 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 221 / 2000 \n",
      " - time: 7.084235906600952 - sq_loss: 1.947157215909101e-05 - tot_loss: 0.1116767482344585 - acc: 0.955 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 222 / 2000 \n",
      " - time: 7.154282331466675 - sq_loss: 1.9362942111911252e-05 - tot_loss: 0.11120101314227214 - acc: 0.955 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 223 / 2000 \n",
      " - time: 7.290944814682007 - sq_loss: 1.9253913706052117e-05 - tot_loss: 0.11072965167659277 - acc: 0.955 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 224 / 2000 \n",
      " - time: 7.1801042556762695 - sq_loss: 1.91458693734603e-05 - tot_loss: 0.11026248750749801 - acc: 0.955 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 225 / 2000 \n",
      " - time: 7.186964750289917 - sq_loss: 1.9045006411033683e-05 - tot_loss: 0.10979935816540091 - acc: 0.9551666666666667 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 226 / 2000 \n",
      " - time: 7.175567388534546 - sq_loss: 1.8941538655781187e-05 - tot_loss: 0.10934036520084191 - acc: 0.9551666666666667 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 227 / 2000 \n",
      " - time: 7.050440549850464 - sq_loss: 1.884136872831732e-05 - tot_loss: 0.10888548297225498 - acc: 0.9553333333333334 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 228 / 2000 \n",
      " - time: 7.196614980697632 - sq_loss: 1.8741036910796538e-05 - tot_loss: 0.10843464089775808 - acc: 0.9553333333333334 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 229 / 2000 \n",
      " - time: 7.176374197006226 - sq_loss: 1.8646333046490327e-05 - tot_loss: 0.1079879638586135 - acc: 0.9553333333333334 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 230 / 2000 \n",
      " - time: 7.237095355987549 - sq_loss: 1.8549479136709124e-05 - tot_loss: 0.10754536863605609 - acc: 0.9555 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 231 / 2000 \n",
      " - time: 7.090651035308838 - sq_loss: 1.8452592485118657e-05 - tot_loss: 0.10710665286605944 - acc: 0.9556666666666667 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 232 / 2000 \n",
      " - time: 7.221382141113281 - sq_loss: 1.83555566763971e-05 - tot_loss: 0.10667173057809123 - acc: 0.9556666666666667 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 233 / 2000 \n",
      " - time: 7.200916290283203 - sq_loss: 1.8258921045344323e-05 - tot_loss: 0.10624068709294078 - acc: 0.9556666666666667 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 234 / 2000 \n",
      " - time: 7.247549533843994 - sq_loss: 1.8163043932872824e-05 - tot_loss: 0.10581366936567065 - acc: 0.9558333333333333 - val_acc: 0.944944944944945\n",
      "Repeatition 1 Epoch 235 / 2000 \n",
      " - time: 7.162180662155151 - sq_loss: 1.8066832126351073e-05 - tot_loss: 0.10539042376767611 - acc: 0.9558333333333333 - val_acc: 0.9459459459459459\n",
      "Repeatition 1 Epoch 236 / 2000 \n",
      " - time: 7.204379320144653 - sq_loss: 1.7973294234252535e-05 - tot_loss: 0.1049708038179233 - acc: 0.9561666666666667 - val_acc: 0.9459459459459459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeatition 1 Epoch 237 / 2000 \n",
      " - time: 7.153028964996338 - sq_loss: 1.788163535820786e-05 - tot_loss: 0.10455487470717344 - acc: 0.9563333333333334 - val_acc: 0.9459459459459459\n",
      "Repeatition 1 Epoch 238 / 2000 \n",
      " - time: 7.2952046394348145 - sq_loss: 1.779248304956127e-05 - tot_loss: 0.10414254947063455 - acc: 0.9563333333333334 - val_acc: 0.9459459459459459\n",
      "Repeatition 1 Epoch 239 / 2000 \n",
      " - time: 7.112657785415649 - sq_loss: 1.770337621564977e-05 - tot_loss: 0.10373383870901308 - acc: 0.9563333333333334 - val_acc: 0.9459459459459459\n"
     ]
    }
   ],
   "source": [
    "niter = 2000\n",
    "rank = 220\n",
    "tau = 0.2\n",
    "gamma = 0.5\n",
    "alpha = 1\n",
    "rho = 0.5\n",
    "\n",
    "print (\"rank=\",rank, \"tau=\",tau, \"gamma=\",gamma, \"rho=\",rho, \"alpha\",alpha)\n",
    "\n",
    "loss1 = np.empty(niter)\n",
    "loss2 = np.empty(niter)\n",
    "accuracy_train = np.empty(niter)\n",
    "accuracy_test = np.empty(niter)\n",
    "time1 = np.empty(niter)\n",
    "\n",
    "results = torch.zeros(1, 5, niter)\n",
    "\n",
    "S = 32 ### number of filters 2^5\n",
    "H, W, C, n = x_trainTensor.size()   # n is the same thing as N\n",
    "Hprime = torch.floor(torch.tensor((H-filter_size)/stride))+1\n",
    "Hprime = Hprime.to(torch.int)\n",
    "Wprime = torch.floor(torch.tensor((W-filter_size)/stride))+1\n",
    "Wprime = Wprime.to(torch.int)\n",
    "\n",
    "\n",
    "\n",
    "for Out_iter in range(1):\n",
    "    rank_initial = 700\n",
    "    seed = 10 + 10*Out_iter\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    d1 =  Hprime * Wprime * S\n",
    "    d2 =  1024\n",
    "    d3 =  1024\n",
    "    d4 =  10\n",
    "\n",
    "\n",
    "    W1 = 0.01*torch.randn(filter_size * filter_size * C, S, device=device) \n",
    "    b1 = 0*torch.ones(d1, 1, device=device) # 0 is stable\n",
    "\n",
    "\n",
    "    W2 = init.kaiming_uniform_(torch.empty(d2, d1, device=device),a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    W2_torch_tensor = W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2)) # 2^10 and Hprime * Wprime  *2^5*1^3\n",
    "    W2_tl_tensor = tl.tensor(W2_torch_tensor.cpu().numpy())\n",
    "    factors2 = tensor_train(W2_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial,1))\n",
    "    W2_tl_tensor_rec = tt_to_tensor(factors2)\n",
    "    b2 = 0*torch.ones(d2, 1, device=device)\n",
    "\n",
    "\n",
    "    W3 = init.kaiming_uniform_(torch.empty(d3, d2, device=device),a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    W3_torch_tensor = W3.reshape((4,4,4,4,4,4,4,4,4,4)) # 8 number of 4s, 2 number of 8s\n",
    "    W3_tl_tensor = tl.tensor(W3_torch_tensor.cpu().numpy())\n",
    "    factors3 = tensor_train(W3_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial,1))\n",
    "    W3_tl_tensor_rec = tt_to_tensor(factors3)\n",
    "    b3 = 0*torch.ones(d3, 1, device=device)\n",
    "\n",
    "    W4 = init.kaiming_uniform_(torch.empty(d4, d3, device=device),a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    b4 = 0*torch.ones(d4, 1, device=device)\n",
    "\n",
    "    x_trainTS1 = torch.reshape(x_trainTS, (-1,n))\n",
    "    x_trainTS1 = torch.t(x_trainTS1)\n",
    "    x_trainTS1 = torch.reshape(x_trainTS1, (-1, filter_size * filter_size * C))   ### this is X-bar-bar'\n",
    "    U1prime = torch.matmul(x_trainTS1, W1)\n",
    "    U1prime = torch.reshape(U1prime, (n,-1))\n",
    "    U1 = torch.t(U1prime)\n",
    "\n",
    "    V1 = nn.ReLU()(U1)\n",
    "    U2 = torch.addmm(b2.repeat(1, N), W2, V1)\n",
    "    V2 = nn.ReLU()(U2)\n",
    "    U3 = torch.addmm(b3.repeat(1, N), W3, V2)\n",
    "    V3 = nn.ReLU()(U3)\n",
    "    U4 = torch.addmm(b4.repeat(1, N), W4, V3)\n",
    "    V4 = U4\n",
    "\n",
    "    x_testTS1 = torch.reshape(x_testTS, (-1, N_test))\n",
    "    x_testTS1 = torch.t(x_testTS1)\n",
    "    x_testTS1 = torch.reshape(x_testTS1, (-1, filter_size * filter_size * C))   ### this is X-bar-bar' test\n",
    "\n",
    "\n",
    "    # Iterations\n",
    "    print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
    "    for k in range(niter):\n",
    "        start = time.time()\n",
    "\n",
    "  # update for last layer\n",
    "        # update V4\n",
    "        V4 = (y_one_hot + gamma*U4 + alpha*V4)/(1 + gamma + alpha)\n",
    "\n",
    "        # update U4\n",
    "        U4 = (gamma*V4 + rho*(torch.mm(W4,V3) + b4.repeat(1,N)))/(gamma + rho)\n",
    "\n",
    "        # update W4 and b4\n",
    "        W4, b4 = updateWb_org(U4,V3,W4,b4,alpha,rho)\n",
    "\n",
    "\n",
    "  # update for 3nd layer\n",
    "        # update V3\n",
    "        V3 = updateV(U3,U4,W4,b4,rho,gamma)\n",
    "\n",
    "        # update U3\n",
    "        U3 = relu_prox(V3,(rho*torch.addmm(b3.repeat(1,N), W3, V2) + alpha*U3)/(rho + alpha),(rho + alpha)/gamma,d3,N)\n",
    "\n",
    "        # update W3 and b3\n",
    "        W3, b3 = updateWb(U3,V2,W3,b3,W3_tl_tensor_rec, alpha,rho,tau)\n",
    "\n",
    "        # G update (TTD)\n",
    "        W3_torch_tensor = W3.reshape((4,4,4,4,4,4,4,4,4,4))\n",
    "        W3_tl_tensor = tl.tensor(W3_torch_tensor.cpu().numpy())  # transfer tensorly package\n",
    "        factors3 = tensor_train(W3_tl_tensor, (1,rank,rank,rank,rank,rank,rank,rank,rank,rank,1))\n",
    "        #set of tensor cores\n",
    "        W3_tl_tensor_rec = tt_to_tensor(factors3)\n",
    "\n",
    "\n",
    "  # update for 2nd layer\n",
    "        # update V2\n",
    "        V2 = updateV(U2,U3,W3,b3,rho,gamma)\n",
    "\n",
    "        # update U2\n",
    "        U2 = relu_prox(V2,(rho*torch.addmm(b2.repeat(1,N), W2, V1) + alpha*U2)/(rho + alpha),(rho + alpha)/gamma,d2,N)\n",
    "\n",
    "        # update W2 and b2\n",
    "        W2, b2 = updateWb(U2,V1,W2,b2,W2_tl_tensor_rec, alpha,rho,tau)\n",
    "\n",
    "        # G update (TTD)\n",
    "        W2_torch_tensor = W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2))\n",
    "        W2_tl_tensor = tl.tensor(W2_torch_tensor.cpu().numpy())\n",
    "        factors2 = tensor_train(W2_tl_tensor, (1,rank,rank,rank,rank,rank,rank,rank,rank,rank,1))\n",
    "        W2_tl_tensor_rec = tt_to_tensor(factors2)\n",
    "\n",
    "\n",
    " # update for 1st layer\n",
    "        # update V1\n",
    "        V1 = updateV(U1,U2,W2,b2,rho,gamma)\n",
    "\n",
    "        XprimeW = torch.reshape(torch.matmul(x_trainTS1, W1), (n,-1))\n",
    "        XprimeWtranspose = torch.t(XprimeW)\n",
    "        # update U1\n",
    "        U1 = relu_prox(V1,(rho*XprimeWtranspose + alpha*U1)/(rho + alpha),(rho + alpha)/gamma,d1,N)\n",
    "\n",
    "        # update W1 and b1\n",
    "        W1 = updateWb_CNNorg(U1,x_trainTS1,W1,alpha,rho)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # prediction for trainning data\n",
    "        XprimeW = torch.reshape(torch.matmul(x_trainTS1, torch.as_tensor(W1,device=device).reshape((filter_size * filter_size * C, S)).float()), (n,-1))\n",
    "        XprimeWtranspose = torch.t(XprimeW)\n",
    "        a1_train = nn.ReLU()(XprimeWtranspose)\n",
    "        a2_train = nn.ReLU()(torch.addmm(b2.repeat(1, N), torch.as_tensor(W2_tl_tensor_rec,device=device).reshape((d2, d1)).float(), a1_train))\n",
    "        a3_train = nn.ReLU()(torch.addmm(b3.repeat(1, N), torch.as_tensor(W3_tl_tensor_rec,device=device).reshape((d3, d2)), a2_train))\n",
    "        pred = torch.argmax(torch.addmm(b4.repeat(1, N), W4, a3_train), dim=0)\n",
    "\n",
    "  #Prediction for test data\n",
    "        XprimeWtest = torch.reshape(torch.matmul(x_testTS1, torch.as_tensor(W1,device=device).reshape((filter_size * filter_size * C, S)).float()), (N_test,-1))\n",
    "        XprimeWtesttranspose = torch.t(XprimeWtest)\n",
    "        a1_test = nn.ReLU()(XprimeWtesttranspose) \n",
    "        a2_test = nn.ReLU()(torch.addmm(b2.repeat(1, N_test), torch.as_tensor(W2_tl_tensor_rec,device=device).reshape((d2, d1)).float(), a1_test))\n",
    "        a3_test = nn.ReLU()(torch.addmm(b3.repeat(1, N_test), torch.as_tensor(W3_tl_tensor_rec,device=device).reshape((d3, d2)), a2_test))\n",
    "        pred_test = torch.argmax(torch.addmm(b4.repeat(1, N_test), W4, a3_test), dim=0)\n",
    "\n",
    "\n",
    "    #emperical loss\n",
    "        loss1[k] = gamma/2*torch.pow(torch.dist(V4,y_one_hot,2),2).cpu().numpy()\n",
    "\n",
    "        # Eq (5) in paper\n",
    "        loss2[k] = loss1[k] + rho/2*torch.pow(torch.dist(XprimeWtranspose,U1,2),2).cpu().numpy() \\\n",
    "        +rho/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
    "        +rho/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy() \\\n",
    "        +rho/2*torch.pow(torch.dist(torch.addmm(b4.repeat(1,N), W4, V3),U4,2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V1,nn.ReLU()(U1),2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V2,nn.ReLU()(U2),2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V3,nn.ReLU()(U3),2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V4,U4,2),2).cpu().numpy() \\\n",
    "        +tau/2*torch.pow(torch.dist(W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2)),torch.as_tensor(W2_tl_tensor_rec,device=device).float(),2),2).cpu().numpy() \\\n",
    "        +tau/2*torch.pow(torch.dist(W3.reshape((4,4,4,4,4,4,4,4,4,4)),torch.as_tensor(W3_tl_tensor_rec,device=device).float(),2),2).cpu().numpy() \\\n",
    "\n",
    "        # compute training accuracy\n",
    "        correct_train = pred == y_train\n",
    "        accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
    "\n",
    "        # compute validation accuracy\n",
    "        correct_test = pred_test == y_test\n",
    "        accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
    "\n",
    "        # compute training time\n",
    "        stop = time.time()\n",
    "        duration = stop - start\n",
    "        time1[k] = duration\n",
    "\n",
    "        # print results\n",
    "        print('Repeatition', Out_iter + 1, 'Epoch', k + 1, '/', niter, '\\n',\n",
    "              '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:', loss2[k],\n",
    "              '-', 'acc:', accuracy_train[k], '-', 'val_acc:', accuracy_test[k])\n",
    "\n",
    "##############\n",
    "############## compute CR\n",
    "    factors2_shape=[f.shape for f in factors2]\n",
    "    Sum_of_variables_factors2=sum(list(x*y*z for x,y,z in factors2_shape))\n",
    "    factors3_shape=[f.shape for f in factors3]\n",
    "    Sum_of_variables_factors3=sum(list(x*y*z for x,y,z in factors3_shape))\n",
    "    total_variabels=Sum_of_variables_factors2+Sum_of_variables_factors3\n",
    "\n",
    "    layer2_CR = Sum_of_variables_factors2/(d1*d2).item()\n",
    "    layer3_CR = Sum_of_variables_factors3/(d2*d3)\n",
    "    Compressedlayers_CR = total_variabels/(d1*d2+d2*d3).item()\n",
    "    Compressedlayers_CR2 = (total_variabels+d3*d4)/(d1*d2+d2*d3+d3*d4).item()\n",
    "\n",
    "\n",
    "\n",
    "    results[Out_iter,0,:] = torch.tensor(loss1)\n",
    "    results[Out_iter,1,:] = torch.tensor(loss2)\n",
    "    results[Out_iter,2,:] = torch.tensor(accuracy_train)\n",
    "    results[Out_iter,3,:] = torch.tensor(accuracy_test)\n",
    "    results[Out_iter,4,:] = torch.tensor(time1)\n",
    "    CR=(layer2_CR,layer3_CR,Compressedlayers_CR,Compressedlayers_CR2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338341d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
