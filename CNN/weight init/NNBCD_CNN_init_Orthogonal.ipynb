{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016eae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.13.1\n",
      "Torchvision Version: 0.14.1\n",
      "GPU is available? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly.decomposition import tensor_train\n",
    "from tensorly.decomposition import tensor_train\n",
    "from tensorly import tt_to_tensor\n",
    "from tensorly.decomposition import matrix_product_state\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Torchvision Version:\", torchvision.__version__)\n",
    "print(\"GPU is available?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e2af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert to tensor and scale to [0, 1]\n",
    "ts = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
    "mnist_trainset = datasets.MNIST('/home/c/cl237/', train=True, download=True, transform=ts)\n",
    "mnist_testset = datasets.MNIST(root='/home/c/cl237/', train=False, download=True, transform=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4dbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "train_subset_size = int(0.1 * len(mnist_trainset))\n",
    "test_subset_size = int(0.0999 * len(mnist_testset))\n",
    "train_indices = list(range(len(mnist_trainset)))\n",
    "test_indices = list(range(len(mnist_testset)))\n",
    "torch.manual_seed(10)\n",
    "\n",
    "seed_value = 10\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "np.random.shuffle(train_indices)\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "train_subset_indices = train_indices[:train_subset_size]\n",
    "test_subset_indices = test_indices[:test_subset_size]\n",
    "\n",
    "mnist_trainset = Subset(mnist_trainset, train_subset_indices)\n",
    "mnist_testset = Subset(mnist_testset, test_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4bb9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d0 = mnist_trainset[0][0].size()[0]\n",
    "x_d1 = mnist_trainset[0][0].size()[1]\n",
    "x_d2 = mnist_trainset[0][0].size()[2]\n",
    "# ([1, 28, 28])\n",
    "N = x_d3 = len(mnist_trainset)\n",
    "K = 10\n",
    "x_train = torch.empty((N,x_d0*x_d1*x_d2), device=device)\n",
    "# (60000, 28*28)\n",
    "\n",
    "y_train = torch.empty(N, dtype=torch.long)\n",
    "\n",
    "#torch.empty() return a tensor filled with garbage values Dimention: N by x_d0*x_d1*x_d2\n",
    "\n",
    "for i in range(N):\n",
    "     x_train[i,:] = torch.reshape(mnist_trainset[i][0], (1, x_d0*x_d1*x_d2))\n",
    "     y_train[i] = mnist_trainset[i][1]\n",
    "x_train = torch.t(x_train)\n",
    "y_one_hot = torch.zeros(N, K).scatter_(1, torch.reshape(y_train, (N, 1)), 1)\n",
    "y_one_hot = torch.t(y_one_hot).to(device=device)\n",
    "y_train = y_train.to(device=device)\n",
    "\n",
    "# Manipulate test set\n",
    "N_test = x_d3_test = len(mnist_testset)\n",
    "x_test = torch.empty((N_test,x_d0*x_d1*x_d2), device=device)\n",
    "y_test = torch.empty(N_test, dtype=torch.long)\n",
    "for i in range(N_test):\n",
    "     x_test[i,:] = torch.reshape(mnist_testset[i][0], (1, x_d0*x_d1*x_d2))\n",
    "     y_test[i] = mnist_testset[i][1]\n",
    "x_test = torch.t(x_test)\n",
    "y_test_one_hot = torch.zeros(N_test, K).scatter_(1, torch.reshape(y_test, (N_test, 1)), 1)\n",
    "y_test_one_hot = torch.t(y_test_one_hot).to(device=device)\n",
    "y_test = y_test.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbabbe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28, 1, 6000])\n",
      "torch.Size([28, 28, 1, 999])\n"
     ]
    }
   ],
   "source": [
    "#### Reshape X to X-bar\n",
    "x_trainTensor = torch.reshape(x_train, (x_d1, x_d2, x_d0,-1))\n",
    "x_testTensor  = torch.reshape(x_test, (x_d1, x_d2, x_d0,-1))\n",
    "print(x_trainTensor.shape)\n",
    "print(x_testTensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e198ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Chenyang: you need to write a function to transform the x_trainTensor, x_testTensor to (H'W')*(l^2C)*n\n",
    "#### follow the paper in https://arxiv.org/pdf/1611.03214.pdf   this paper donot have the stride, we should also include stride\n",
    "#### how to make it faster;  make sure the code is correct?\n",
    "def inputX_CNN(x_Tensor, filter_size, stride):\n",
    "    H, W, C, n = x_Tensor.size()\n",
    "    Hprime = torch.floor(torch.tensor((H-filter_size)/stride))+1\n",
    "    Hprime = Hprime.to(torch.int)\n",
    "    Wprime = torch.floor(torch.tensor((W-filter_size)/stride))+1\n",
    "    Wprime = Wprime.to(torch.int)\n",
    "    Xtranform = torch.zeros((Hprime * Wprime, filter_size * filter_size * C, n), device=device)\n",
    "    for i in range(n):\n",
    "      Data =  x_Tensor[:,:,:,i]\n",
    "      for hh in range(Hprime):\n",
    "        for ww in range(Wprime):\n",
    "          #  print(range(ww * stride, ww * stride + filter_size))\n",
    "            DataTemp = Data[range(hh * stride, hh * stride + filter_size), :,:]\n",
    "            DataTemp = DataTemp[:, range(ww * stride, ww * stride + filter_size), :]\n",
    "            Xtranform[Hprime * hh + ww, :, i] = torch.reshape(DataTemp, (1, filter_size * filter_size * C))\n",
    "    return Xtranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### from X-bar to X-bar-bar\n",
    "filter_size=3\n",
    "stride = 2\n",
    "x_trainTS = inputX_CNN(x_trainTensor,filter_size,stride)\n",
    "x_testTS  = inputX_CNN(x_testTensor,filter_size,stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c49c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To-do-list\n",
    "#### Chenyang: you need to write a function to update W for CNN\n",
    "def updateWb_CNN(U, V, W, W_tensor_rec, alpha, rho,tau): # W:previous W\n",
    "    W_tensor_rec = torch.as_tensor(W_tensor_rec,device=device).float()\n",
    "    W_tensor2matrix = W_tensor_rec.reshape(W.shape)  #w shape need to take a look at\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(N, device=device)\n",
    "    U_prime = torch.t(U).reshape(n*Hprime*Wprime,-1)\n",
    "    # _, col_U = U_prime.size()\n",
    "    Wstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(V),V))+(alpha+tau)*I), rho*torch.mm(torch.t(V),U_prime)+alpha*W+tau*W_tensor2matrix)\n",
    "    # bstar = 0\n",
    "    return Wstar\n",
    "# def updateWb_CNN(U, V, W, W_tensor_rec, alpha, rho,tau): # W:previous W, v for the first layer should be X-bar-bar-prime\n",
    "#     W_tensor_rec = torch.as_tensor(W_tensor_rec,device=device).float()\n",
    "#     W_tensor2matrix = W_tensor_rec.reshape(W.shape)  #w shape need to take a look at\n",
    "#     d,N = V.size()\n",
    "#     I = torch.eye(N, device=device)\n",
    "#     U_prime = torch.t(U).reshape(n*Hprime*Wprime,-1)\n",
    "#     A = rho*torch.mm(torch.t(V),U_prime)+alpha*W+tau*W_tensor2matrix\n",
    "#     A = torch.t(A)\n",
    "#     # _, col_U = U_prime.size()\n",
    "#     Wstar_transpose = torch.mm(A, torch.inverse(rho*(torch.mm(torch.t(V), V))+(alpha+tau)*I))\n",
    "#     Wstar =  torch.t(Wstar_transpose)\n",
    "#     return Wstar\n",
    "\n",
    "def updateWb_CNNorg(U, V, W, alpha, rho):\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(N, device=device)\n",
    "    #_, col_U = U.size()\n",
    "    U_prime = torch.t(U).reshape(n*Hprime*Wprime,-1)\n",
    "    Wstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(V),V))+alpha*I), rho*torch.mm(torch.t(V),U_prime)+alpha*W)\n",
    "    \n",
    "    return Wstar\n",
    "\n",
    "def updateV(U1,U2,W,b,rho,gamma):\n",
    "    _, d = W.size()\n",
    "    I = torch.eye(d, device=device) #Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "    U1 = nn.ReLU()(U1)\n",
    "    _, col_U2 = U2.size()\n",
    "    Vstar = torch.mm(torch.inverse(rho*(torch.mm(torch.t(W),W))+gamma*I), rho*torch.mm(torch.t(W),U2-b.repeat(1,col_U2))+gamma*U1)\n",
    "    # check how to get V* W*\n",
    "    # torch.mm*=(input,mat2) performs a matrix multiplication of the matrices input and mat2.\n",
    "    #   if input is a (n*m) tensor,mat2 is a (m*p) tensor, output will be (n*p) tensor\n",
    "    return Vstar\n",
    "\n",
    "def updateWb_org(U, V, W, b, alpha, rho):\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(d, device=device)\n",
    "    _, col_U = U.size()\n",
    "    Wstar = torch.mm(alpha*W+rho*torch.mm(U-b.repeat(1,col_U),torch.t(V)),torch.inverse(alpha*I+rho*(torch.mm(V,torch.t(V)))))\n",
    "    bstar = 0*(alpha*b+rho*torch.sum(U-torch.mm(Wstar,V), dim=1).reshape(b.size()))/(rho*N+alpha)\n",
    "    return Wstar, bstar\n",
    "\n",
    "def updateWb(U, V, W, b, W_tensor_rec, alpha, rho,tau):\n",
    "    W_tensor_rec = torch.as_tensor(W_tensor_rec,device=device).float()\n",
    "    W_tensor2matrix = W_tensor_rec.reshape(W.shape)\n",
    "    d,N = V.size()\n",
    "    I = torch.eye(d, device=device)\n",
    "    _, col_U = U.size()\n",
    "    Wstar = torch.mm(alpha*W+tau*W_tensor2matrix+rho*torch.mm(U-b.repeat(1,col_U),torch.t(V)),torch.inverse((alpha+tau)*I+rho*(torch.mm(V,torch.t(V)))))\n",
    "    bstar = 0*(alpha*b+rho*torch.sum(U-torch.mm(Wstar,V), dim=1).reshape(b.size()))/(rho*N+alpha)\n",
    "    return Wstar, bstar\n",
    "\n",
    "# U update  E(1) in global convergent\n",
    "def relu_prox(a, b, gamma, d, N):\n",
    "    val = torch.empty(d,N, device=device)\n",
    "    x = (a+gamma*b)/(1+gamma)\n",
    "    y = torch.min(b,torch.zeros(d,N, device=device))\n",
    "\n",
    "    val = torch.where(a+gamma*b < 0, y, torch.zeros(d,N, device=device))\n",
    "    val = torch.where(((a+gamma*b >= 0) & (b >=0)) | ((a*(gamma-np.sqrt(gamma*(gamma+1))) <= gamma*b) & (b < 0)), x, val)\n",
    "    val = torch.where((-a <= gamma*b) & (gamma*b <= a*(gamma-np.sqrt(gamma*(gamma+1)))), b, val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### parameter setup\n",
    "#df = pd.DataFrame()\n",
    "#df.to_csv('C:/Users/Mark/Desktop/CNNmnist_hyperparameter_test.csv')\n",
    "niter = 2000\n",
    "rank = 220\n",
    "tau = 0.2\n",
    "gamma = 0.5\n",
    "alpha = 1\n",
    "rho = 0.5\n",
    "\n",
    "print (\"rank=\",rank, \"tau=\",tau, \"gamma=\",gamma, \"rho=\",rho, \"alpha\",alpha)\n",
    "\n",
    "loss1 = np.empty(niter)\n",
    "loss2 = np.empty(niter)\n",
    "accuracy_train = np.empty(niter)\n",
    "accuracy_test = np.empty(niter)\n",
    "time1 = np.empty(niter)\n",
    "\n",
    "results = torch.zeros(1, 5, niter)\n",
    "\n",
    "S = 32 ### number of filters 2^5\n",
    "H, W, C, n = x_trainTensor.size()   # n is the same thing as N\n",
    "Hprime = torch.floor(torch.tensor((H-filter_size)/stride))+1\n",
    "Hprime = Hprime.to(torch.int)\n",
    "Wprime = torch.floor(torch.tensor((W-filter_size)/stride))+1\n",
    "Wprime = Wprime.to(torch.int)\n",
    "\n",
    "\n",
    "\n",
    "for Out_iter in range(1):\n",
    "    rank_initial = 700\n",
    "    seed = 10 + 10*Out_iter\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    ## Chenyang\n",
    "    d1 =  Hprime * Wprime * S\n",
    "    d2 =  1024\n",
    "    d3 =  1024\n",
    "    d4 =  10\n",
    "\n",
    "\n",
    "    W1 = 0.01*torch.randn(filter_size * filter_size * C, S, device=device)  ## How people usually initialize the CNN kernel?\n",
    "    #W1_torch_tensor = W1.reshape((filter_size, filter_size, C, 2,2,2,2,2)) ## TBD\n",
    "    #W1_tl_tensor = tl.tensor(W1_torch_tensor.cpu().numpy())\n",
    "    #factors1 = tensor_train(W1_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, 1))\n",
    "    #W1_tl_tensor_rec = tt_to_tensor(factors1)\n",
    "    b1 = 0*torch.ones(d1, 1, device=device) # 0 is stable\n",
    "\n",
    "\n",
    "    W2 = init.orthogonal_(torch.empty(d2, d1, device=device),gain=1)\n",
    "    W2_torch_tensor = W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2)) # 2^10 and Hprime * Wprime  *2^5*1^3\n",
    "    W2_tl_tensor = tl.tensor(W2_torch_tensor.cpu().numpy())\n",
    "    factors2 = tensor_train(W2_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial,1))\n",
    "    # we have 9 rank_initial (r1,...,r9) here plus 2 of 1s (r0 & r10).\n",
    "    # factors: set of tensor cores\n",
    "    # http://tensorly.org/stable/user_guide/tensor_decomposition.html\n",
    "    # http://tensorly.org/stable/modules/generated/tensorly.decomposition.tensor_train.html#tensorly.decomposition.tensor_train\n",
    "    W2_tl_tensor_rec = tt_to_tensor(factors2)\n",
    "    # tt_to_tensor(factors):Re-assembles ‘factors’, which represent a tensor in TT format into the corresponding full tensor\n",
    "    #      facros: list of 3d-arrays tt-cores           output_tensor: ndarray\n",
    "    b2 = 0*torch.ones(d2, 1, device=device)\n",
    "\n",
    "\n",
    "    W3 = init.orthogonal_(torch.empty(d3, d2, device=device),gain=1)\n",
    "    W3_torch_tensor = W3.reshape((4,4,4,4,4,4,4,4,4,4)) # 8 number of 4s, 2 number of 8s\n",
    "    W3_tl_tensor = tl.tensor(W3_torch_tensor.cpu().numpy())\n",
    "    factors3 = tensor_train(W3_tl_tensor, (1, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial, rank_initial,1))\n",
    "    W3_tl_tensor_rec = tt_to_tensor(factors3)\n",
    "    b3 = 0*torch.ones(d3, 1, device=device)\n",
    "\n",
    "    W4 = init.orthogonal_(torch.empty(d4, d3, device=device),gain=1)\n",
    "    b4 = 0*torch.ones(d4, 1, device=device)\n",
    "\n",
    "    ## Chenyang: this is for the initialization of U1\n",
    "    x_trainTS1 = torch.reshape(x_trainTS, (-1,n))\n",
    "    x_trainTS1 = torch.t(x_trainTS1)\n",
    "    x_trainTS1 = torch.reshape(x_trainTS1, (-1, filter_size * filter_size * C))   ### this is X-bar-bar'\n",
    "    U1prime = torch.matmul(x_trainTS1, W1)\n",
    "    U1prime = torch.reshape(U1prime, (n,-1))\n",
    "    U1 = torch.t(U1prime)\n",
    "\n",
    "    V1 = nn.ReLU()(U1)\n",
    "    U2 = torch.addmm(b2.repeat(1, N), W2, V1)\n",
    "    V2 = nn.ReLU()(U2)\n",
    "    U3 = torch.addmm(b3.repeat(1, N), W3, V2)\n",
    "    V3 = nn.ReLU()(U3)\n",
    "    U4 = torch.addmm(b4.repeat(1, N), W4, V3)\n",
    "    V4 = U4\n",
    "\n",
    "    ### to create the X-bar-bar' for test data\n",
    "    x_testTS1 = torch.reshape(x_testTS, (-1, N_test))\n",
    "    x_testTS1 = torch.t(x_testTS1)\n",
    "    x_testTS1 = torch.reshape(x_testTS1, (-1, filter_size * filter_size * C))   ### this is X-bar-bar' test\n",
    "\n",
    "\n",
    "    # Iterations\n",
    "    print('Train on', N, 'samples, validate on', N_test, 'samples')\n",
    "    for k in range(niter):\n",
    "        start = time.time()\n",
    "\n",
    "  # update for last layer\n",
    "        # update V4\n",
    "        V4 = (y_one_hot + gamma*U4 + alpha*V4)/(1 + gamma + alpha)\n",
    "\n",
    "        # update U4\n",
    "        U4 = (gamma*V4 + rho*(torch.mm(W4,V3) + b4.repeat(1,N)))/(gamma + rho)\n",
    "\n",
    "        # update W4 and b4\n",
    "        W4, b4 = updateWb_org(U4,V3,W4,b4,alpha,rho)\n",
    "        # W3_torch_tensor = W3.reshape((d3,4,4,4,4,4,4))\n",
    "        # W3_tl_tensor = tl.tensor(W3_torch_tensor.cpu().numpy())\n",
    "        # factors3 = tensor_train(W3_tl_tensor, (1,rank,rank,rank,rank,rank,rank,1))\n",
    "        # W3_tl_tensor_rec = tt_to_tensor(factors3)\n",
    "\n",
    "\n",
    "  # update for 3nd layer\n",
    "        # update V3\n",
    "        V3 = updateV(U3,U4,W4,b4,rho,gamma)\n",
    "\n",
    "        # update U3\n",
    "        U3 = relu_prox(V3,(rho*torch.addmm(b3.repeat(1,N), W3, V2) + alpha*U3)/(rho + alpha),(rho + alpha)/gamma,d3,N)\n",
    "\n",
    "        # update W3 and b3\n",
    "        W3, b3 = updateWb(U3,V2,W3,b3,W3_tl_tensor_rec, alpha,rho,tau)\n",
    "\n",
    "        # G update (TTD)\n",
    "        W3_torch_tensor = W3.reshape((4,4,4,4,4,4,4,4,4,4))\n",
    "        W3_tl_tensor = tl.tensor(W3_torch_tensor.cpu().numpy())  # transfer tensorly package\n",
    "        factors3 = tensor_train(W3_tl_tensor, (1,rank,rank,rank,rank,rank,rank,rank,rank,rank,1))\n",
    "        #set of tensor cores\n",
    "        W3_tl_tensor_rec = tt_to_tensor(factors3)\n",
    "\n",
    "\n",
    "  # update for 2nd layer\n",
    "        # update V2\n",
    "        V2 = updateV(U2,U3,W3,b3,rho,gamma)\n",
    "\n",
    "        # update U2\n",
    "        U2 = relu_prox(V2,(rho*torch.addmm(b2.repeat(1,N), W2, V1) + alpha*U2)/(rho + alpha),(rho + alpha)/gamma,d2,N)\n",
    "\n",
    "        # update W2 and b2\n",
    "        W2, b2 = updateWb(U2,V1,W2,b2,W2_tl_tensor_rec, alpha,rho,tau)\n",
    "\n",
    "        # G update (TTD)\n",
    "        W2_torch_tensor = W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2))\n",
    "        W2_tl_tensor = tl.tensor(W2_torch_tensor.cpu().numpy())\n",
    "        factors2 = tensor_train(W2_tl_tensor, (1,rank,rank,rank,rank,rank,rank,rank,rank,rank,1))\n",
    "        W2_tl_tensor_rec = tt_to_tensor(factors2)\n",
    "\n",
    "\n",
    " # update for 1st layer\n",
    "        # update V1\n",
    "        V1 = updateV(U1,U2,W2,b2,rho,gamma)\n",
    "\n",
    "        ## Chenyang\n",
    "        XprimeW = torch.reshape(torch.matmul(x_trainTS1, W1), (n,-1))\n",
    "        XprimeWtranspose = torch.t(XprimeW)\n",
    "        # update U1\n",
    "        U1 = relu_prox(V1,(rho*XprimeWtranspose + alpha*U1)/(rho + alpha),(rho + alpha)/gamma,d1,N)\n",
    "\n",
    "        # update W1 and b1\n",
    "        ## Chenyang: you need to use the new function updateWb_CNN\n",
    "        W1 = updateWb_CNNorg(U1,x_trainTS1,W1,alpha,rho)\n",
    "\n",
    "        # G update\n",
    "        #W1_torch_tensor = W1.reshape((filter_size, filter_size, C, 2,2,2,2,2))\n",
    "        #W1_tl_tensor = tl.tensor(W1_torch_tensor.cpu().numpy())\n",
    "        #factors1 = tensor_train(W1_tl_tensor, (1,rank,rank,rank,rank,rank,rank,rank,1))\n",
    "        #W1_tl_tensor_rec = tt_to_tensor(factors1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # prediction for trainning data\n",
    "        ### we should use reconstructed tensnor W1 for prediction\n",
    "        XprimeW = torch.reshape(torch.matmul(x_trainTS1, torch.as_tensor(W1,device=device).reshape((filter_size * filter_size * C, S)).float()), (n,-1))\n",
    "        XprimeWtranspose = torch.t(XprimeW)\n",
    "        a1_train = nn.ReLU()(XprimeWtranspose)\n",
    "        #a1_train = nn.ReLU()(torch.addmm(b1.repeat(1, N), W1, x_train))\n",
    "        a2_train = nn.ReLU()(torch.addmm(b2.repeat(1, N), torch.as_tensor(W2_tl_tensor_rec,device=device).reshape((d2, d1)).float(), a1_train))\n",
    "        a3_train = nn.ReLU()(torch.addmm(b3.repeat(1, N), torch.as_tensor(W3_tl_tensor_rec,device=device).reshape((d3, d2)), a2_train))\n",
    "        #print(torch.addmm(b4.repeat(1, N), W4, a3_train))\n",
    "        pred = torch.argmax(torch.addmm(b4.repeat(1, N), W4, a3_train), dim=0)\n",
    "        # check argmax and addmm, dim=0\n",
    "\n",
    "  #Prediction for test data\n",
    "        XprimeWtest = torch.reshape(torch.matmul(x_testTS1, torch.as_tensor(W1,device=device).reshape((filter_size * filter_size * C, S)).float()), (N_test,-1))\n",
    "        XprimeWtesttranspose = torch.t(XprimeWtest)\n",
    "        a1_test = nn.ReLU()(XprimeWtesttranspose) ## Chenyang\n",
    "        #a1_test = nn.ReLU()(torch.addmm(b1.repeat(1, N_test), W1, x_test))\n",
    "        a2_test = nn.ReLU()(torch.addmm(b2.repeat(1, N_test), torch.as_tensor(W2_tl_tensor_rec,device=device).reshape((d2, d1)).float(), a1_test))\n",
    "        a3_test = nn.ReLU()(torch.addmm(b3.repeat(1, N_test), torch.as_tensor(W3_tl_tensor_rec,device=device).reshape((d3, d2)), a2_test))\n",
    "        pred_test = torch.argmax(torch.addmm(b4.repeat(1, N_test), W4, a3_test), dim=0)\n",
    "\n",
    "\n",
    "    #emperical loss\n",
    "        loss1[k] = gamma/2*torch.pow(torch.dist(V4,y_one_hot,2),2).cpu().numpy()\n",
    "\n",
    "        # Eq (5) in paper\n",
    "        ## Chenyang\n",
    "        loss2[k] = loss1[k] + rho/2*torch.pow(torch.dist(XprimeWtranspose,U1,2),2).cpu().numpy() \\\n",
    "        +rho/2*torch.pow(torch.dist(torch.addmm(b2.repeat(1,N), W2, V1),U2,2),2).cpu().numpy() \\\n",
    "        +rho/2*torch.pow(torch.dist(torch.addmm(b3.repeat(1,N), W3, V2),U3,2),2).cpu().numpy() \\\n",
    "        +rho/2*torch.pow(torch.dist(torch.addmm(b4.repeat(1,N), W4, V3),U4,2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V1,nn.ReLU()(U1),2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V2,nn.ReLU()(U2),2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V3,nn.ReLU()(U3),2),2).cpu().numpy() \\\n",
    "        + gamma/2*torch.pow(torch.dist(V4,U4,2),2).cpu().numpy() \\\n",
    "        +tau/2*torch.pow(torch.dist(W2.reshape((2*Hprime,2*Wprime,4,4,4,4,4,2,2,2)),torch.as_tensor(W2_tl_tensor_rec,device=device).float(),2),2).cpu().numpy() \\\n",
    "        +tau/2*torch.pow(torch.dist(W3.reshape((4,4,4,4,4,4,4,4,4,4)),torch.as_tensor(W3_tl_tensor_rec,device=device).float(),2),2).cpu().numpy() \\\n",
    "        # +tau/2*torch.pow(torch.dist(W3.reshape((d3,4,4,4,4,4,4)),torch.as_tensor(W3_tl_tensor_rec,device=device).float(),2),2).cpu().numpy()\n",
    "        # +tau/2*torch.pow(torch.dist(W4.reshape((50,30,K)),torch.as_tensor(W4_tl_tensor_rec,device=device),2),2).cpu().numpy()\n",
    "\n",
    "        # compute training accuracy\n",
    "        correct_train = pred == y_train\n",
    "        accuracy_train[k] = np.mean(correct_train.cpu().numpy())\n",
    "\n",
    "        # compute validation accuracy\n",
    "        correct_test = pred_test == y_test\n",
    "        accuracy_test[k] = np.mean(correct_test.cpu().numpy())\n",
    "\n",
    "        # compute training time\n",
    "        stop = time.time()\n",
    "        duration = stop - start\n",
    "        time1[k] = duration\n",
    "\n",
    "        # print results\n",
    "        print('Repeatition', Out_iter + 1, 'Epoch', k + 1, '/', niter, '\\n',\n",
    "              '-', 'time:', time1[k], '-', 'sq_loss:', loss1[k], '-', 'tot_loss:', loss2[k],\n",
    "              '-', 'acc:', accuracy_train[k], '-', 'val_acc:', accuracy_test[k])\n",
    "\n",
    "##############\n",
    "############## compute CR\n",
    "    # factors1_shape=[f.shape for f in factors1]\n",
    "    # Sum_of_variables_factors1=sum(list(x*y*z for x,y,z in factors1_shape))\n",
    "    factors2_shape=[f.shape for f in factors2]\n",
    "    Sum_of_variables_factors2=sum(list(x*y*z for x,y,z in factors2_shape))\n",
    "    factors3_shape=[f.shape for f in factors3]\n",
    "    Sum_of_variables_factors3=sum(list(x*y*z for x,y,z in factors3_shape))\n",
    "    total_variabels=Sum_of_variables_factors2+Sum_of_variables_factors3\n",
    "\n",
    "    layer2_CR = Sum_of_variables_factors2/(d1*d2).item()\n",
    "    layer3_CR = Sum_of_variables_factors3/(d2*d3)\n",
    "    Compressedlayers_CR = total_variabels/(d1*d2+d2*d3).item()\n",
    "    Compressedlayers_CR2 = (total_variabels+d3*d4)/(d1*d2+d2*d3+d3*d4).item()\n",
    "\n",
    "    # print(\"CR_1 = \"+str(CR_1)+\"   \"+\"CR_2 = \" +str(CR_2))\n",
    "\n",
    "\n",
    "    results[Out_iter,0,:] = torch.tensor(loss1)\n",
    "    results[Out_iter,1,:] = torch.tensor(loss2)\n",
    "    results[Out_iter,2,:] = torch.tensor(accuracy_train)\n",
    "    results[Out_iter,3,:] = torch.tensor(accuracy_test)\n",
    "    results[Out_iter,4,:] = torch.tensor(time1)\n",
    "    CR=(layer2_CR,layer3_CR,Compressedlayers_CR,Compressedlayers_CR2)\n",
    "\n",
    "    #this postion to add new row into existing table\n",
    "    #df=pd.read_csv('C:/Users/Mark/Desktop/CNNmnist_hyperparameter_test.csv')\n",
    "    #new_row = {'rank':rank, 'tau':tau, 'gamma':gamma, 'rho':rho, 'alpha':alpha, \\\n",
    "    #           'loss1':loss1[niter-1], 'loss2':loss2[niter-1], 'accuracy_train':accuracy_train[niter-1], \\\n",
    "    #           'accuracy_test':accuracy_test[niter-1], 'time':time1[niter-1], 'layer2_CR':layer2_CR, \\\n",
    "    #          'layer3_CR':layer3_CR, 'Compressedlayers_CR':Compressedlayers_CR}\n",
    "    #df=df.append(new_row,ignore_index=True)\n",
    "    #df.to_csv('C:/Users/Mark/Desktop/CNNmnist_hyperparameter_test.csv',index=False)\n",
    "\n",
    "\n",
    "filename=\"Orthogonal_\"+ \"niter_\"+ str(niter) + \"rank_\" + str(rank) + \"tau_\" + str(tau) + \"gamma_\" + str(gamma) + \\\n",
    "\"rho_\" + str(rho) + \"alpha_\" + str(alpha) + \".mat\"\n",
    "from scipy.io import savemat\n",
    "%cd /home/c/cl237/TenBCDCNN_mnist_init\n",
    "savemat (filename, {'results': torch.Tensor.numpy(results), 'Compression Ratio':CR})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353f750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
